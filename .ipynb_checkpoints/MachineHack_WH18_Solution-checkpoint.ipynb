{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MachineHack Weekend Hackathon 18\n",
    "\n",
    "https://www.machinehack.com/hackathons/detecting_anomalies_in_wafer_manufacturing_weekend_hackathon_18\n",
    "\n",
    "\n",
    "\n",
    "# Overview\n",
    "\n",
    "Detecting Anomalies can be a difficult task and especially in the case of labeled datasets due to some level of human bias introduced while labeling the final product as anomalous or good. These giant manufacturing systems need to be monitored every 10 milliseconds to capture their behavior which brings in lots of information and what we call the Industrial IoT (IIOT). Also, hardly a manufacturer wants to create an anomalous product. Hence, the anomalies are like a needle in a haystack which renders the dataset that is significantly Imbalanced. \n",
    "\n",
    "Capturing such a dataset using a machine learning model and making the model generalize can be fun. In this competition, we bring such a use-case from one of India's leading manufacturers of wafers(semiconductors). The dataset collected was anonymized to hide the feature names, also there are 1558 features that would require some serious domain knowledge to understand them. \n",
    "\n",
    "However, In the era of Deep Learning, we are challenging the data science community to come up with an anomaly detection model that can generalize well on the unseen set of data(Test data). In this hackathon, you will be creating a machine learning/ deep learning model to classify the anomalies correctly using Area under the curve(AUC) as metric.\n",
    "\n",
    " \n",
    "\n",
    "## Dataset Description:\n",
    "\n",
    "Train.csv - 1763 rows x 1559 columns\n",
    "\n",
    "Test.csv - 756 rows x 1558 columns\n",
    "\n",
    "Sample Submission.csv - Please check the Evaluation section for more details on how to generate a valid submission\n",
    " \n",
    "\n",
    "### Attribute Description:\n",
    "\n",
    "Feature_1 - Feature_1558 - Represents the various attributes that were collected from the manufacturing machine\n",
    "\n",
    "Class - (0 or 1) - Represents Good/Anomalous class labels for the products\n",
    " \n",
    "\n",
    "## Skills:\n",
    "\n",
    "High Dimensionality Data, Overfitting-vs-Underfitting\n",
    "\n",
    "Advanced Classification Techniques, Gradient Boosting, Neural Nets, etc\n",
    "\n",
    "Feature engineering, Feature Selection Techniques\n",
    "\n",
    "Optimizing Area under the curve(AUC) to generalize well on unseen data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature_1</th>\n",
       "      <th>feature_2</th>\n",
       "      <th>feature_3</th>\n",
       "      <th>feature_4</th>\n",
       "      <th>feature_5</th>\n",
       "      <th>feature_6</th>\n",
       "      <th>feature_7</th>\n",
       "      <th>feature_8</th>\n",
       "      <th>feature_9</th>\n",
       "      <th>feature_10</th>\n",
       "      <th>...</th>\n",
       "      <th>feature_1550</th>\n",
       "      <th>feature_1551</th>\n",
       "      <th>feature_1552</th>\n",
       "      <th>feature_1553</th>\n",
       "      <th>feature_1554</th>\n",
       "      <th>feature_1555</th>\n",
       "      <th>feature_1556</th>\n",
       "      <th>feature_1557</th>\n",
       "      <th>feature_1558</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100</td>\n",
       "      <td>160</td>\n",
       "      <td>1.6000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20</td>\n",
       "      <td>83</td>\n",
       "      <td>4.1500</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>99</td>\n",
       "      <td>150</td>\n",
       "      <td>1.5151</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>40</td>\n",
       "      <td>40</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12</td>\n",
       "      <td>234</td>\n",
       "      <td>19.5000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1758</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1759</th>\n",
       "      <td>40</td>\n",
       "      <td>200</td>\n",
       "      <td>5.0000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1760</th>\n",
       "      <td>96</td>\n",
       "      <td>218</td>\n",
       "      <td>2.2708</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1761</th>\n",
       "      <td>16</td>\n",
       "      <td>81</td>\n",
       "      <td>5.0625</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1762</th>\n",
       "      <td>36</td>\n",
       "      <td>41</td>\n",
       "      <td>1.1388</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1763 rows × 1559 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      feature_1  feature_2  feature_3  feature_4  feature_5  feature_6  \\\n",
       "0           100        160     1.6000          0          0          0   \n",
       "1            20         83     4.1500          1          0          0   \n",
       "2            99        150     1.5151          1          0          0   \n",
       "3            40         40     1.0000          0          0          0   \n",
       "4            12        234    19.5000          1          0          0   \n",
       "...         ...        ...        ...        ...        ...        ...   \n",
       "1758          1          1     2.0000          1          0          0   \n",
       "1759         40        200     5.0000          1          0          0   \n",
       "1760         96        218     2.2708          1          0          0   \n",
       "1761         16         81     5.0625          1          0          0   \n",
       "1762         36         41     1.1388          0          0          0   \n",
       "\n",
       "      feature_7  feature_8  feature_9  feature_10  ...  feature_1550  \\\n",
       "0             0          0          0           0  ...             0   \n",
       "1             0          0          0           1  ...             0   \n",
       "2             0          0          0           0  ...             0   \n",
       "3             0          0          0           0  ...             0   \n",
       "4             0          0          0           0  ...             0   \n",
       "...         ...        ...        ...         ...  ...           ...   \n",
       "1758          0          0          0           0  ...             0   \n",
       "1759          0          0          0           0  ...             0   \n",
       "1760          0          0          0           0  ...             0   \n",
       "1761          1          0          0           0  ...             0   \n",
       "1762          0          0          0           0  ...             0   \n",
       "\n",
       "      feature_1551  feature_1552  feature_1553  feature_1554  feature_1555  \\\n",
       "0                0             0             0             0             0   \n",
       "1                0             0             0             0             1   \n",
       "2                0             0             0             0             0   \n",
       "3                0             0             0             0             0   \n",
       "4                0             0             0             0             0   \n",
       "...            ...           ...           ...           ...           ...   \n",
       "1758             0             0             0             0             0   \n",
       "1759             0             0             0             0             0   \n",
       "1760             0             0             0             0             0   \n",
       "1761             0             0             0             0             0   \n",
       "1762             0             0             0             0             0   \n",
       "\n",
       "      feature_1556  feature_1557  feature_1558  Class  \n",
       "0                0             0             0      0  \n",
       "1                0             0             0      0  \n",
       "2                0             0             0      0  \n",
       "3                0             0             0      0  \n",
       "4                0             0             0      0  \n",
       "...            ...           ...           ...    ...  \n",
       "1758             0             0             0      0  \n",
       "1759             0             0             0      0  \n",
       "1760             0             0             0      0  \n",
       "1761             0             0             0      0  \n",
       "1762             0             0             0      0  \n",
       "\n",
       "[1763 rows x 1559 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = pd.read_csv('Train.csv')\n",
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1763, 1559)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1763 entries, 0 to 1762\n",
      "Columns: 1559 entries, feature_1 to Class\n",
      "dtypes: float64(1), int64(1558)\n",
      "memory usage: 21.0 MB\n"
     ]
    }
   ],
   "source": [
    "train_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>feature_1</th>\n",
       "      <td>1763.0</td>\n",
       "      <td>53.094158</td>\n",
       "      <td>55.842014</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>12.00</td>\n",
       "      <td>39.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>640.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_2</th>\n",
       "      <td>1763.0</td>\n",
       "      <td>126.587067</td>\n",
       "      <td>129.859641</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>33.50</td>\n",
       "      <td>96.0</td>\n",
       "      <td>159.0</td>\n",
       "      <td>640.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_3</th>\n",
       "      <td>1763.0</td>\n",
       "      <td>3.423940</td>\n",
       "      <td>4.566858</td>\n",
       "      <td>0.0015</td>\n",
       "      <td>1.25</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>60.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_4</th>\n",
       "      <td>1763.0</td>\n",
       "      <td>0.724334</td>\n",
       "      <td>0.446976</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_5</th>\n",
       "      <td>1763.0</td>\n",
       "      <td>0.002836</td>\n",
       "      <td>0.053194</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_1555</th>\n",
       "      <td>1763.0</td>\n",
       "      <td>0.015315</td>\n",
       "      <td>0.122837</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_1556</th>\n",
       "      <td>1763.0</td>\n",
       "      <td>0.014748</td>\n",
       "      <td>0.120575</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_1557</th>\n",
       "      <td>1763.0</td>\n",
       "      <td>0.009643</td>\n",
       "      <td>0.097750</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_1558</th>\n",
       "      <td>1763.0</td>\n",
       "      <td>0.001134</td>\n",
       "      <td>0.033672</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Class</th>\n",
       "      <td>1763.0</td>\n",
       "      <td>0.081112</td>\n",
       "      <td>0.273084</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1559 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               count        mean         std     min    25%   50%    75%  \\\n",
       "feature_1     1763.0   53.094158   55.842014  1.0000  12.00  39.0   75.0   \n",
       "feature_2     1763.0  126.587067  129.859641  1.0000  33.50  96.0  159.0   \n",
       "feature_3     1763.0    3.423940    4.566858  0.0015   1.25   2.0    4.5   \n",
       "feature_4     1763.0    0.724334    0.446976  0.0000   0.00   1.0    1.0   \n",
       "feature_5     1763.0    0.002836    0.053194  0.0000   0.00   0.0    0.0   \n",
       "...              ...         ...         ...     ...    ...   ...    ...   \n",
       "feature_1555  1763.0    0.015315    0.122837  0.0000   0.00   0.0    0.0   \n",
       "feature_1556  1763.0    0.014748    0.120575  0.0000   0.00   0.0    0.0   \n",
       "feature_1557  1763.0    0.009643    0.097750  0.0000   0.00   0.0    0.0   \n",
       "feature_1558  1763.0    0.001134    0.033672  0.0000   0.00   0.0    0.0   \n",
       "Class         1763.0    0.081112    0.273084  0.0000   0.00   0.0    0.0   \n",
       "\n",
       "                max  \n",
       "feature_1     640.0  \n",
       "feature_2     640.0  \n",
       "feature_3      60.0  \n",
       "feature_4       1.0  \n",
       "feature_5       1.0  \n",
       "...             ...  \n",
       "feature_1555    1.0  \n",
       "feature_1556    1.0  \n",
       "feature_1557    1.0  \n",
       "feature_1558    1.0  \n",
       "Class           1.0  \n",
       "\n",
       "[1559 rows x 8 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['feature_1', 'feature_2', 'feature_3', 'feature_4', 'feature_5',\n",
       "       'feature_6', 'feature_7', 'feature_8', 'feature_9', 'feature_10',\n",
       "       ...\n",
       "       'feature_1550', 'feature_1551', 'feature_1552', 'feature_1553',\n",
       "       'feature_1554', 'feature_1555', 'feature_1556', 'feature_1557',\n",
       "       'feature_1558', 'Class'],\n",
       "      dtype='object', length=1559)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "features = train_data.drop('Class',axis = 1)\n",
    "target = train_data['Class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.84021237,  0.25737336, -0.39949961, ..., -0.12234515,\n",
       "        -0.09867391, -0.03370042],\n",
       "       [-0.59280719, -0.33574276,  0.15902958, ..., -0.12234515,\n",
       "        -0.09867391, -0.03370042],\n",
       "       [ 0.82229963,  0.1803453 , -0.41809535, ..., -0.12234515,\n",
       "        -0.09867391, -0.03370042],\n",
       "       ...,\n",
       "       [ 0.76856139,  0.70413616, -0.25257358, ..., -0.12234515,\n",
       "        -0.09867391, -0.03370042],\n",
       "       [-0.66445817, -0.35114837,  0.35889541, ..., -0.12234515,\n",
       "        -0.09867391, -0.03370042],\n",
       "       [-0.30620328, -0.65926065, -0.50051673, ..., -0.12234515,\n",
       "        -0.09867391, -0.03370042]])"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#x = train_d.loc[:, features].values\n",
    "x = StandardScaler().fit_transform(features) # normalizing the features\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature_1</th>\n",
       "      <th>feature_2</th>\n",
       "      <th>feature_3</th>\n",
       "      <th>feature_4</th>\n",
       "      <th>feature_5</th>\n",
       "      <th>feature_6</th>\n",
       "      <th>feature_7</th>\n",
       "      <th>feature_8</th>\n",
       "      <th>feature_9</th>\n",
       "      <th>feature_10</th>\n",
       "      <th>...</th>\n",
       "      <th>feature_1549</th>\n",
       "      <th>feature_1550</th>\n",
       "      <th>feature_1551</th>\n",
       "      <th>feature_1552</th>\n",
       "      <th>feature_1553</th>\n",
       "      <th>feature_1554</th>\n",
       "      <th>feature_1555</th>\n",
       "      <th>feature_1556</th>\n",
       "      <th>feature_1557</th>\n",
       "      <th>feature_1558</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.840212</td>\n",
       "      <td>0.257373</td>\n",
       "      <td>-0.399500</td>\n",
       "      <td>-1.620979</td>\n",
       "      <td>-0.05333</td>\n",
       "      <td>-0.05333</td>\n",
       "      <td>-0.089468</td>\n",
       "      <td>-0.047687</td>\n",
       "      <td>-0.067516</td>\n",
       "      <td>-0.112412</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.071632</td>\n",
       "      <td>-0.063137</td>\n",
       "      <td>-0.075528</td>\n",
       "      <td>-0.05333</td>\n",
       "      <td>-0.063137</td>\n",
       "      <td>-0.063137</td>\n",
       "      <td>-0.124712</td>\n",
       "      <td>-0.122345</td>\n",
       "      <td>-0.098674</td>\n",
       "      <td>-0.0337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.592807</td>\n",
       "      <td>-0.335743</td>\n",
       "      <td>0.159030</td>\n",
       "      <td>0.616911</td>\n",
       "      <td>-0.05333</td>\n",
       "      <td>-0.05333</td>\n",
       "      <td>-0.089468</td>\n",
       "      <td>-0.047687</td>\n",
       "      <td>-0.067516</td>\n",
       "      <td>8.895862</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.071632</td>\n",
       "      <td>-0.063137</td>\n",
       "      <td>-0.075528</td>\n",
       "      <td>-0.05333</td>\n",
       "      <td>-0.063137</td>\n",
       "      <td>-0.063137</td>\n",
       "      <td>8.018497</td>\n",
       "      <td>-0.122345</td>\n",
       "      <td>-0.098674</td>\n",
       "      <td>-0.0337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.822300</td>\n",
       "      <td>0.180345</td>\n",
       "      <td>-0.418095</td>\n",
       "      <td>0.616911</td>\n",
       "      <td>-0.05333</td>\n",
       "      <td>-0.05333</td>\n",
       "      <td>-0.089468</td>\n",
       "      <td>-0.047687</td>\n",
       "      <td>-0.067516</td>\n",
       "      <td>-0.112412</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.071632</td>\n",
       "      <td>-0.063137</td>\n",
       "      <td>-0.075528</td>\n",
       "      <td>-0.05333</td>\n",
       "      <td>-0.063137</td>\n",
       "      <td>-0.063137</td>\n",
       "      <td>-0.124712</td>\n",
       "      <td>-0.122345</td>\n",
       "      <td>-0.098674</td>\n",
       "      <td>-0.0337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.234552</td>\n",
       "      <td>-0.666963</td>\n",
       "      <td>-0.530918</td>\n",
       "      <td>-1.620979</td>\n",
       "      <td>-0.05333</td>\n",
       "      <td>-0.05333</td>\n",
       "      <td>-0.089468</td>\n",
       "      <td>-0.047687</td>\n",
       "      <td>-0.067516</td>\n",
       "      <td>-0.112412</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.071632</td>\n",
       "      <td>-0.063137</td>\n",
       "      <td>-0.075528</td>\n",
       "      <td>-0.05333</td>\n",
       "      <td>-0.063137</td>\n",
       "      <td>-0.063137</td>\n",
       "      <td>-0.124712</td>\n",
       "      <td>-0.122345</td>\n",
       "      <td>-0.098674</td>\n",
       "      <td>-0.0337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.736109</td>\n",
       "      <td>0.827381</td>\n",
       "      <td>3.521156</td>\n",
       "      <td>0.616911</td>\n",
       "      <td>-0.05333</td>\n",
       "      <td>-0.05333</td>\n",
       "      <td>-0.089468</td>\n",
       "      <td>-0.047687</td>\n",
       "      <td>-0.067516</td>\n",
       "      <td>-0.112412</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.071632</td>\n",
       "      <td>-0.063137</td>\n",
       "      <td>-0.075528</td>\n",
       "      <td>-0.05333</td>\n",
       "      <td>-0.063137</td>\n",
       "      <td>-0.063137</td>\n",
       "      <td>-0.124712</td>\n",
       "      <td>-0.122345</td>\n",
       "      <td>-0.098674</td>\n",
       "      <td>-0.0337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1758</th>\n",
       "      <td>-0.933149</td>\n",
       "      <td>-0.967373</td>\n",
       "      <td>-0.311887</td>\n",
       "      <td>0.616911</td>\n",
       "      <td>-0.05333</td>\n",
       "      <td>-0.05333</td>\n",
       "      <td>-0.089468</td>\n",
       "      <td>-0.047687</td>\n",
       "      <td>-0.067516</td>\n",
       "      <td>-0.112412</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.071632</td>\n",
       "      <td>-0.063137</td>\n",
       "      <td>-0.075528</td>\n",
       "      <td>-0.05333</td>\n",
       "      <td>-0.063137</td>\n",
       "      <td>-0.063137</td>\n",
       "      <td>-0.124712</td>\n",
       "      <td>-0.122345</td>\n",
       "      <td>-0.098674</td>\n",
       "      <td>-0.0337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1759</th>\n",
       "      <td>-0.234552</td>\n",
       "      <td>0.565486</td>\n",
       "      <td>0.345206</td>\n",
       "      <td>0.616911</td>\n",
       "      <td>-0.05333</td>\n",
       "      <td>-0.05333</td>\n",
       "      <td>-0.089468</td>\n",
       "      <td>-0.047687</td>\n",
       "      <td>-0.067516</td>\n",
       "      <td>-0.112412</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.071632</td>\n",
       "      <td>-0.063137</td>\n",
       "      <td>-0.075528</td>\n",
       "      <td>-0.05333</td>\n",
       "      <td>-0.063137</td>\n",
       "      <td>-0.063137</td>\n",
       "      <td>-0.124712</td>\n",
       "      <td>-0.122345</td>\n",
       "      <td>-0.098674</td>\n",
       "      <td>-0.0337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1760</th>\n",
       "      <td>0.768561</td>\n",
       "      <td>0.704136</td>\n",
       "      <td>-0.252574</td>\n",
       "      <td>0.616911</td>\n",
       "      <td>-0.05333</td>\n",
       "      <td>-0.05333</td>\n",
       "      <td>-0.089468</td>\n",
       "      <td>-0.047687</td>\n",
       "      <td>-0.067516</td>\n",
       "      <td>-0.112412</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.071632</td>\n",
       "      <td>-0.063137</td>\n",
       "      <td>-0.075528</td>\n",
       "      <td>-0.05333</td>\n",
       "      <td>-0.063137</td>\n",
       "      <td>-0.063137</td>\n",
       "      <td>-0.124712</td>\n",
       "      <td>-0.122345</td>\n",
       "      <td>-0.098674</td>\n",
       "      <td>-0.0337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1761</th>\n",
       "      <td>-0.664458</td>\n",
       "      <td>-0.351148</td>\n",
       "      <td>0.358895</td>\n",
       "      <td>0.616911</td>\n",
       "      <td>-0.05333</td>\n",
       "      <td>-0.05333</td>\n",
       "      <td>11.177145</td>\n",
       "      <td>-0.047687</td>\n",
       "      <td>-0.067516</td>\n",
       "      <td>-0.112412</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.071632</td>\n",
       "      <td>-0.063137</td>\n",
       "      <td>-0.075528</td>\n",
       "      <td>-0.05333</td>\n",
       "      <td>-0.063137</td>\n",
       "      <td>-0.063137</td>\n",
       "      <td>-0.124712</td>\n",
       "      <td>-0.122345</td>\n",
       "      <td>-0.098674</td>\n",
       "      <td>-0.0337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1762</th>\n",
       "      <td>-0.306203</td>\n",
       "      <td>-0.659261</td>\n",
       "      <td>-0.500517</td>\n",
       "      <td>-1.620979</td>\n",
       "      <td>-0.05333</td>\n",
       "      <td>-0.05333</td>\n",
       "      <td>-0.089468</td>\n",
       "      <td>-0.047687</td>\n",
       "      <td>-0.067516</td>\n",
       "      <td>-0.112412</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.071632</td>\n",
       "      <td>-0.063137</td>\n",
       "      <td>-0.075528</td>\n",
       "      <td>-0.05333</td>\n",
       "      <td>-0.063137</td>\n",
       "      <td>-0.063137</td>\n",
       "      <td>-0.124712</td>\n",
       "      <td>-0.122345</td>\n",
       "      <td>-0.098674</td>\n",
       "      <td>-0.0337</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1763 rows × 1558 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      feature_1  feature_2  feature_3  feature_4  feature_5  feature_6  \\\n",
       "0      0.840212   0.257373  -0.399500  -1.620979   -0.05333   -0.05333   \n",
       "1     -0.592807  -0.335743   0.159030   0.616911   -0.05333   -0.05333   \n",
       "2      0.822300   0.180345  -0.418095   0.616911   -0.05333   -0.05333   \n",
       "3     -0.234552  -0.666963  -0.530918  -1.620979   -0.05333   -0.05333   \n",
       "4     -0.736109   0.827381   3.521156   0.616911   -0.05333   -0.05333   \n",
       "...         ...        ...        ...        ...        ...        ...   \n",
       "1758  -0.933149  -0.967373  -0.311887   0.616911   -0.05333   -0.05333   \n",
       "1759  -0.234552   0.565486   0.345206   0.616911   -0.05333   -0.05333   \n",
       "1760   0.768561   0.704136  -0.252574   0.616911   -0.05333   -0.05333   \n",
       "1761  -0.664458  -0.351148   0.358895   0.616911   -0.05333   -0.05333   \n",
       "1762  -0.306203  -0.659261  -0.500517  -1.620979   -0.05333   -0.05333   \n",
       "\n",
       "      feature_7  feature_8  feature_9  feature_10  ...  feature_1549  \\\n",
       "0     -0.089468  -0.047687  -0.067516   -0.112412  ...     -0.071632   \n",
       "1     -0.089468  -0.047687  -0.067516    8.895862  ...     -0.071632   \n",
       "2     -0.089468  -0.047687  -0.067516   -0.112412  ...     -0.071632   \n",
       "3     -0.089468  -0.047687  -0.067516   -0.112412  ...     -0.071632   \n",
       "4     -0.089468  -0.047687  -0.067516   -0.112412  ...     -0.071632   \n",
       "...         ...        ...        ...         ...  ...           ...   \n",
       "1758  -0.089468  -0.047687  -0.067516   -0.112412  ...     -0.071632   \n",
       "1759  -0.089468  -0.047687  -0.067516   -0.112412  ...     -0.071632   \n",
       "1760  -0.089468  -0.047687  -0.067516   -0.112412  ...     -0.071632   \n",
       "1761  11.177145  -0.047687  -0.067516   -0.112412  ...     -0.071632   \n",
       "1762  -0.089468  -0.047687  -0.067516   -0.112412  ...     -0.071632   \n",
       "\n",
       "      feature_1550  feature_1551  feature_1552  feature_1553  feature_1554  \\\n",
       "0        -0.063137     -0.075528      -0.05333     -0.063137     -0.063137   \n",
       "1        -0.063137     -0.075528      -0.05333     -0.063137     -0.063137   \n",
       "2        -0.063137     -0.075528      -0.05333     -0.063137     -0.063137   \n",
       "3        -0.063137     -0.075528      -0.05333     -0.063137     -0.063137   \n",
       "4        -0.063137     -0.075528      -0.05333     -0.063137     -0.063137   \n",
       "...            ...           ...           ...           ...           ...   \n",
       "1758     -0.063137     -0.075528      -0.05333     -0.063137     -0.063137   \n",
       "1759     -0.063137     -0.075528      -0.05333     -0.063137     -0.063137   \n",
       "1760     -0.063137     -0.075528      -0.05333     -0.063137     -0.063137   \n",
       "1761     -0.063137     -0.075528      -0.05333     -0.063137     -0.063137   \n",
       "1762     -0.063137     -0.075528      -0.05333     -0.063137     -0.063137   \n",
       "\n",
       "      feature_1555  feature_1556  feature_1557  feature_1558  \n",
       "0        -0.124712     -0.122345     -0.098674       -0.0337  \n",
       "1         8.018497     -0.122345     -0.098674       -0.0337  \n",
       "2        -0.124712     -0.122345     -0.098674       -0.0337  \n",
       "3        -0.124712     -0.122345     -0.098674       -0.0337  \n",
       "4        -0.124712     -0.122345     -0.098674       -0.0337  \n",
       "...            ...           ...           ...           ...  \n",
       "1758     -0.124712     -0.122345     -0.098674       -0.0337  \n",
       "1759     -0.124712     -0.122345     -0.098674       -0.0337  \n",
       "1760     -0.124712     -0.122345     -0.098674       -0.0337  \n",
       "1761     -0.124712     -0.122345     -0.098674       -0.0337  \n",
       "1762     -0.124712     -0.122345     -0.098674       -0.0337  \n",
       "\n",
       "[1763 rows x 1558 columns]"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalised_features = pd.DataFrame(x,columns=features.columns)\n",
    "normalised_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-5.58048275e-01, -7.16515303e-01, -3.18898153e-01, ...,\n",
       "         1.00436083e-02, -3.48615099e-01,  1.98484054e-01],\n",
       "       [ 8.37737033e-01,  2.77879803e+00,  4.51599610e+01, ...,\n",
       "         4.57595123e-01, -8.31939702e-01,  8.61167854e-01],\n",
       "       [-5.82241311e-01, -8.19751315e-01, -3.68210145e-01, ...,\n",
       "         8.05339351e-02, -5.50994158e-02, -2.73801742e-02],\n",
       "       ...,\n",
       "       [-1.43146754e-01, -5.17879549e-01, -2.03924769e-01, ...,\n",
       "         9.45138996e-02, -3.03213651e-01, -2.58729720e-01],\n",
       "       [-5.58648959e-01, -7.40465151e-01, -2.37189291e-01, ...,\n",
       "         1.35620441e-01, -1.23209676e-01, -5.65702008e-01],\n",
       "       [-4.15518876e-01, -1.52689587e+00, -5.78465170e-01, ...,\n",
       "        -1.85562190e+00, -2.01074467e+00,  3.89007660e+00]])"
      ]
     },
     "execution_count": 289,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca_train_data = PCA(n_components=25)\n",
    "principalComponents_train_data = pca_train_data.fit_transform(x)\n",
    "principalComponents_train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.558048</td>\n",
       "      <td>-0.716515</td>\n",
       "      <td>-0.318898</td>\n",
       "      <td>-0.206731</td>\n",
       "      <td>-0.169782</td>\n",
       "      <td>-0.037985</td>\n",
       "      <td>0.136408</td>\n",
       "      <td>0.455660</td>\n",
       "      <td>-1.643362</td>\n",
       "      <td>0.510866</td>\n",
       "      <td>...</td>\n",
       "      <td>0.407705</td>\n",
       "      <td>-0.916642</td>\n",
       "      <td>-0.492703</td>\n",
       "      <td>-0.335181</td>\n",
       "      <td>-0.047846</td>\n",
       "      <td>-0.319799</td>\n",
       "      <td>0.410761</td>\n",
       "      <td>0.010044</td>\n",
       "      <td>-0.348615</td>\n",
       "      <td>0.198484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.837737</td>\n",
       "      <td>2.778798</td>\n",
       "      <td>45.159961</td>\n",
       "      <td>-3.646199</td>\n",
       "      <td>-0.029670</td>\n",
       "      <td>0.583927</td>\n",
       "      <td>0.053811</td>\n",
       "      <td>-0.724773</td>\n",
       "      <td>1.638031</td>\n",
       "      <td>0.287593</td>\n",
       "      <td>...</td>\n",
       "      <td>1.423503</td>\n",
       "      <td>0.199652</td>\n",
       "      <td>0.370881</td>\n",
       "      <td>-1.077504</td>\n",
       "      <td>-2.187792</td>\n",
       "      <td>4.123523</td>\n",
       "      <td>1.427168</td>\n",
       "      <td>0.457595</td>\n",
       "      <td>-0.831940</td>\n",
       "      <td>0.861168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.582241</td>\n",
       "      <td>-0.819751</td>\n",
       "      <td>-0.368210</td>\n",
       "      <td>-0.197020</td>\n",
       "      <td>0.379266</td>\n",
       "      <td>-0.030617</td>\n",
       "      <td>0.193156</td>\n",
       "      <td>0.621970</td>\n",
       "      <td>-1.999939</td>\n",
       "      <td>0.638865</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.727901</td>\n",
       "      <td>0.921544</td>\n",
       "      <td>-0.012318</td>\n",
       "      <td>0.029063</td>\n",
       "      <td>-0.225241</td>\n",
       "      <td>-0.122732</td>\n",
       "      <td>-0.166258</td>\n",
       "      <td>0.080534</td>\n",
       "      <td>-0.055099</td>\n",
       "      <td>-0.027380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.593072</td>\n",
       "      <td>-0.653044</td>\n",
       "      <td>-0.184739</td>\n",
       "      <td>-0.227168</td>\n",
       "      <td>-0.195470</td>\n",
       "      <td>-0.041866</td>\n",
       "      <td>0.106036</td>\n",
       "      <td>0.354489</td>\n",
       "      <td>-1.247171</td>\n",
       "      <td>0.483833</td>\n",
       "      <td>...</td>\n",
       "      <td>0.382916</td>\n",
       "      <td>-0.833296</td>\n",
       "      <td>0.264780</td>\n",
       "      <td>-0.054057</td>\n",
       "      <td>-0.305493</td>\n",
       "      <td>-0.378308</td>\n",
       "      <td>-0.358172</td>\n",
       "      <td>-0.010193</td>\n",
       "      <td>-0.245576</td>\n",
       "      <td>-0.089439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.232055</td>\n",
       "      <td>-0.552640</td>\n",
       "      <td>-0.045577</td>\n",
       "      <td>-0.079442</td>\n",
       "      <td>-0.156646</td>\n",
       "      <td>-0.021530</td>\n",
       "      <td>0.092923</td>\n",
       "      <td>0.636596</td>\n",
       "      <td>-0.961730</td>\n",
       "      <td>0.194614</td>\n",
       "      <td>...</td>\n",
       "      <td>0.326762</td>\n",
       "      <td>-0.538156</td>\n",
       "      <td>0.207099</td>\n",
       "      <td>0.210538</td>\n",
       "      <td>-0.749217</td>\n",
       "      <td>-0.483319</td>\n",
       "      <td>-0.535025</td>\n",
       "      <td>0.087476</td>\n",
       "      <td>-0.077933</td>\n",
       "      <td>-0.189193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1758</th>\n",
       "      <td>-0.634727</td>\n",
       "      <td>-0.862651</td>\n",
       "      <td>-0.288634</td>\n",
       "      <td>-0.196237</td>\n",
       "      <td>-0.207512</td>\n",
       "      <td>-0.062232</td>\n",
       "      <td>0.074970</td>\n",
       "      <td>0.315069</td>\n",
       "      <td>-1.001584</td>\n",
       "      <td>0.359792</td>\n",
       "      <td>...</td>\n",
       "      <td>0.496431</td>\n",
       "      <td>-1.314711</td>\n",
       "      <td>0.063869</td>\n",
       "      <td>0.209690</td>\n",
       "      <td>-0.943272</td>\n",
       "      <td>-0.300201</td>\n",
       "      <td>-0.552895</td>\n",
       "      <td>0.211515</td>\n",
       "      <td>-0.632590</td>\n",
       "      <td>-0.115938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1759</th>\n",
       "      <td>-0.395848</td>\n",
       "      <td>-0.859442</td>\n",
       "      <td>-0.353699</td>\n",
       "      <td>-0.242991</td>\n",
       "      <td>-0.234461</td>\n",
       "      <td>-0.070808</td>\n",
       "      <td>0.134367</td>\n",
       "      <td>0.589440</td>\n",
       "      <td>-1.939173</td>\n",
       "      <td>0.537665</td>\n",
       "      <td>...</td>\n",
       "      <td>0.646847</td>\n",
       "      <td>-1.750854</td>\n",
       "      <td>-0.377068</td>\n",
       "      <td>0.192892</td>\n",
       "      <td>-0.570580</td>\n",
       "      <td>-0.327897</td>\n",
       "      <td>1.390710</td>\n",
       "      <td>1.314297</td>\n",
       "      <td>-0.862657</td>\n",
       "      <td>-2.161395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1760</th>\n",
       "      <td>-0.143147</td>\n",
       "      <td>-0.517880</td>\n",
       "      <td>-0.203925</td>\n",
       "      <td>-0.099210</td>\n",
       "      <td>-0.123212</td>\n",
       "      <td>-0.043295</td>\n",
       "      <td>0.126928</td>\n",
       "      <td>0.428650</td>\n",
       "      <td>-1.377671</td>\n",
       "      <td>0.266091</td>\n",
       "      <td>...</td>\n",
       "      <td>0.251854</td>\n",
       "      <td>-0.605294</td>\n",
       "      <td>0.195160</td>\n",
       "      <td>0.112145</td>\n",
       "      <td>-0.684311</td>\n",
       "      <td>-0.396460</td>\n",
       "      <td>-0.388438</td>\n",
       "      <td>0.094514</td>\n",
       "      <td>-0.303214</td>\n",
       "      <td>-0.258730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1761</th>\n",
       "      <td>-0.558649</td>\n",
       "      <td>-0.740465</td>\n",
       "      <td>-0.237189</td>\n",
       "      <td>-0.117568</td>\n",
       "      <td>-0.225293</td>\n",
       "      <td>-0.065648</td>\n",
       "      <td>0.127242</td>\n",
       "      <td>0.549807</td>\n",
       "      <td>-1.334187</td>\n",
       "      <td>0.290346</td>\n",
       "      <td>...</td>\n",
       "      <td>0.880484</td>\n",
       "      <td>-1.298150</td>\n",
       "      <td>0.115051</td>\n",
       "      <td>0.280699</td>\n",
       "      <td>-1.325528</td>\n",
       "      <td>-0.976018</td>\n",
       "      <td>-1.109694</td>\n",
       "      <td>0.135620</td>\n",
       "      <td>-0.123210</td>\n",
       "      <td>-0.565702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1762</th>\n",
       "      <td>-0.415519</td>\n",
       "      <td>-1.526896</td>\n",
       "      <td>-0.578465</td>\n",
       "      <td>0.245955</td>\n",
       "      <td>-0.391434</td>\n",
       "      <td>-0.322777</td>\n",
       "      <td>-0.078100</td>\n",
       "      <td>0.359725</td>\n",
       "      <td>-3.420950</td>\n",
       "      <td>1.284168</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.387169</td>\n",
       "      <td>1.194138</td>\n",
       "      <td>-3.592475</td>\n",
       "      <td>-2.809671</td>\n",
       "      <td>4.380297</td>\n",
       "      <td>1.490775</td>\n",
       "      <td>8.976359</td>\n",
       "      <td>-1.855622</td>\n",
       "      <td>-2.010745</td>\n",
       "      <td>3.890077</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1763 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0         1          2         3         4         5         6   \\\n",
       "0    -0.558048 -0.716515  -0.318898 -0.206731 -0.169782 -0.037985  0.136408   \n",
       "1     0.837737  2.778798  45.159961 -3.646199 -0.029670  0.583927  0.053811   \n",
       "2    -0.582241 -0.819751  -0.368210 -0.197020  0.379266 -0.030617  0.193156   \n",
       "3    -0.593072 -0.653044  -0.184739 -0.227168 -0.195470 -0.041866  0.106036   \n",
       "4    -0.232055 -0.552640  -0.045577 -0.079442 -0.156646 -0.021530  0.092923   \n",
       "...        ...       ...        ...       ...       ...       ...       ...   \n",
       "1758 -0.634727 -0.862651  -0.288634 -0.196237 -0.207512 -0.062232  0.074970   \n",
       "1759 -0.395848 -0.859442  -0.353699 -0.242991 -0.234461 -0.070808  0.134367   \n",
       "1760 -0.143147 -0.517880  -0.203925 -0.099210 -0.123212 -0.043295  0.126928   \n",
       "1761 -0.558649 -0.740465  -0.237189 -0.117568 -0.225293 -0.065648  0.127242   \n",
       "1762 -0.415519 -1.526896  -0.578465  0.245955 -0.391434 -0.322777 -0.078100   \n",
       "\n",
       "            7         8         9   ...        15        16        17  \\\n",
       "0     0.455660 -1.643362  0.510866  ...  0.407705 -0.916642 -0.492703   \n",
       "1    -0.724773  1.638031  0.287593  ...  1.423503  0.199652  0.370881   \n",
       "2     0.621970 -1.999939  0.638865  ... -0.727901  0.921544 -0.012318   \n",
       "3     0.354489 -1.247171  0.483833  ...  0.382916 -0.833296  0.264780   \n",
       "4     0.636596 -0.961730  0.194614  ...  0.326762 -0.538156  0.207099   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "1758  0.315069 -1.001584  0.359792  ...  0.496431 -1.314711  0.063869   \n",
       "1759  0.589440 -1.939173  0.537665  ...  0.646847 -1.750854 -0.377068   \n",
       "1760  0.428650 -1.377671  0.266091  ...  0.251854 -0.605294  0.195160   \n",
       "1761  0.549807 -1.334187  0.290346  ...  0.880484 -1.298150  0.115051   \n",
       "1762  0.359725 -3.420950  1.284168  ... -1.387169  1.194138 -3.592475   \n",
       "\n",
       "            18        19        20        21        22        23        24  \n",
       "0    -0.335181 -0.047846 -0.319799  0.410761  0.010044 -0.348615  0.198484  \n",
       "1    -1.077504 -2.187792  4.123523  1.427168  0.457595 -0.831940  0.861168  \n",
       "2     0.029063 -0.225241 -0.122732 -0.166258  0.080534 -0.055099 -0.027380  \n",
       "3    -0.054057 -0.305493 -0.378308 -0.358172 -0.010193 -0.245576 -0.089439  \n",
       "4     0.210538 -0.749217 -0.483319 -0.535025  0.087476 -0.077933 -0.189193  \n",
       "...        ...       ...       ...       ...       ...       ...       ...  \n",
       "1758  0.209690 -0.943272 -0.300201 -0.552895  0.211515 -0.632590 -0.115938  \n",
       "1759  0.192892 -0.570580 -0.327897  1.390710  1.314297 -0.862657 -2.161395  \n",
       "1760  0.112145 -0.684311 -0.396460 -0.388438  0.094514 -0.303214 -0.258730  \n",
       "1761  0.280699 -1.325528 -0.976018 -1.109694  0.135620 -0.123210 -0.565702  \n",
       "1762 -2.809671  4.380297  1.490775  8.976359 -1.855622 -2.010745  3.890077  \n",
       "\n",
       "[1763 rows x 25 columns]"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca_normalised_features = pd.DataFrame(principalComponents_train_data)\n",
    "pca_normalised_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explained variation per principal component: [0.02188486 0.01962761 0.0170717  0.01536517 0.01509653 0.01448713\n",
      " 0.01378712 0.01318886 0.01273219 0.01201977 0.01133953 0.01102462\n",
      " 0.01069774 0.01063819 0.01044874 0.00996767 0.00977277 0.00892582\n",
      " 0.00886465 0.00871982 0.0085213  0.00837459 0.00819937 0.00799044\n",
      " 0.00777366]\n"
     ]
    }
   ],
   "source": [
    "print('Explained variation per principal component: {}'.format(pca_train_data.explained_variance_ratio_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features, validation_features, train_target, validation_target = train_test_split(pca_normalised_features,target,test_size = 0.20, random_state = 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "model = MLPClassifier(hidden_layer_sizes=(50,30,20),activation= 'relu',solver ='adam', batch_size = 30,\n",
    "                      alpha = .01,max_iter=100,learning_rate= 'adaptive',verbose=3, random_state = 12) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.34484071\n",
      "Iteration 2, loss = 0.22899839\n",
      "Iteration 3, loss = 0.21367682\n",
      "Iteration 4, loss = 0.21538479\n",
      "Iteration 5, loss = 0.20940632\n",
      "Iteration 6, loss = 0.20990756\n",
      "Iteration 7, loss = 0.21000401\n",
      "Iteration 8, loss = 0.21181763\n",
      "Iteration 9, loss = 0.20474122\n",
      "Iteration 10, loss = 0.20889592\n",
      "Iteration 11, loss = 0.20516501\n",
      "Iteration 12, loss = 0.20702862\n",
      "Iteration 13, loss = 0.20688293\n",
      "Iteration 14, loss = 0.21143226\n",
      "Iteration 15, loss = 0.20528772\n",
      "Iteration 16, loss = 0.21153295\n",
      "Iteration 17, loss = 0.20306328\n",
      "Iteration 18, loss = 0.20082699\n",
      "Iteration 19, loss = 0.20933037\n",
      "Iteration 20, loss = 0.20428364\n",
      "Iteration 21, loss = 0.20373840\n",
      "Iteration 22, loss = 0.19887185\n",
      "Iteration 23, loss = 0.19891788\n",
      "Iteration 24, loss = 0.20632979\n",
      "Iteration 25, loss = 0.19701057\n",
      "Iteration 26, loss = 0.19601021\n",
      "Iteration 27, loss = 0.19818988\n",
      "Iteration 28, loss = 0.19983043\n",
      "Iteration 29, loss = 0.19649561\n",
      "Iteration 30, loss = 0.19725974\n",
      "Iteration 31, loss = 0.19678370\n",
      "Iteration 32, loss = 0.19432773\n",
      "Iteration 33, loss = 0.19385174\n",
      "Iteration 34, loss = 0.19385278\n",
      "Iteration 35, loss = 0.19712372\n",
      "Iteration 36, loss = 0.20007489\n",
      "Iteration 37, loss = 0.19493211\n",
      "Iteration 38, loss = 0.19471548\n",
      "Iteration 39, loss = 0.19238443\n",
      "Iteration 40, loss = 0.19809467\n",
      "Iteration 41, loss = 0.19550133\n",
      "Iteration 42, loss = 0.19493512\n",
      "Iteration 43, loss = 0.19410144\n",
      "Iteration 44, loss = 0.19344770\n",
      "Iteration 45, loss = 0.19199710\n",
      "Iteration 46, loss = 0.19345983\n",
      "Iteration 47, loss = 0.19595323\n",
      "Iteration 48, loss = 0.19176333\n",
      "Iteration 49, loss = 0.19245376\n",
      "Iteration 50, loss = 0.19260072\n",
      "Iteration 51, loss = 0.19084334\n",
      "Iteration 52, loss = 0.19278801\n",
      "Iteration 53, loss = 0.19519447\n",
      "Iteration 54, loss = 0.18992661\n",
      "Iteration 55, loss = 0.19133283\n",
      "Iteration 56, loss = 0.19237162\n",
      "Iteration 57, loss = 0.19033812\n",
      "Iteration 58, loss = 0.18995901\n",
      "Iteration 59, loss = 0.19025851\n",
      "Iteration 60, loss = 0.19120966\n",
      "Iteration 61, loss = 0.19112800\n",
      "Iteration 62, loss = 0.18952044\n",
      "Iteration 63, loss = 0.19001794\n",
      "Iteration 64, loss = 0.18900879\n",
      "Iteration 65, loss = 0.18829617\n",
      "Iteration 66, loss = 0.18885753\n",
      "Iteration 67, loss = 0.19109037\n",
      "Iteration 68, loss = 0.18854798\n",
      "Iteration 69, loss = 0.18874730\n",
      "Iteration 70, loss = 0.18938519\n",
      "Iteration 71, loss = 0.19110718\n",
      "Iteration 72, loss = 0.19440423\n",
      "Iteration 73, loss = 0.19182520\n",
      "Iteration 74, loss = 0.18921788\n",
      "Iteration 75, loss = 0.18889342\n",
      "Iteration 76, loss = 0.18814706\n",
      "Iteration 77, loss = 0.18783106\n",
      "Iteration 78, loss = 0.18790710\n",
      "Iteration 79, loss = 0.18833439\n",
      "Iteration 80, loss = 0.18813056\n",
      "Iteration 81, loss = 0.18587944\n",
      "Iteration 82, loss = 0.18871784\n",
      "Iteration 83, loss = 0.18802889\n",
      "Iteration 84, loss = 0.18719008\n",
      "Iteration 85, loss = 0.18659531\n",
      "Iteration 86, loss = 0.18671518\n",
      "Iteration 87, loss = 0.18558997\n",
      "Iteration 88, loss = 0.18619469\n",
      "Iteration 89, loss = 0.18480560\n",
      "Iteration 90, loss = 0.18640430\n",
      "Iteration 91, loss = 0.18650837\n",
      "Iteration 92, loss = 0.18582386\n",
      "Iteration 93, loss = 0.18505542\n",
      "Iteration 94, loss = 0.18645806\n",
      "Iteration 95, loss = 0.18513227\n",
      "Iteration 96, loss = 0.18665058\n",
      "Iteration 97, loss = 0.18588212\n",
      "Iteration 98, loss = 0.18621666\n",
      "Iteration 99, loss = 0.18492262\n",
      "Iteration 100, loss = 0.18641331\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(alpha=0.01, batch_size=30, hidden_layer_sizes=(50, 30, 20),\n",
       "              learning_rate='adaptive', max_iter=100, random_state=12,\n",
       "              verbose=3)"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_features,train_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model_predict = model.predict(train_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fpr_train, tpr_train, threshold = roc_curve(train_target, model.predict_proba(train_features)[:,1])\n",
    "roc_auc_train = auc(fpr_train, tpr_train)\n",
    "\n",
    "# image drawing\n",
    "plt.figure()\n",
    "plt.title('Receiver Operating Characteristic ')\n",
    "plt.plot(fpr_train, tpr_train, label = 'MLP AUC = %0.2f' % roc_auc_train)\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYoAAAEWCAYAAAB42tAoAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3dd3hUZfbA8e8hlNCRqoAICiJEinQLKKIoKLoKKIi67qpYwLKKBfvaCxawAaKiwg9UVgUr2LCxrKDSmwgBQpMmAgFJOb8/3hszxGQyJLlzZybn8zzzZO7cduYmmTP3fe89r6gqxhhjTEHKBB2AMcaY2GaJwhhjTFiWKIwxxoRlicIYY0xYliiMMcaEZYnCGGNMWJYoTEREZLGInBJ0HLFCRO4QkXEB7Xu8iDwYxL5LmogMEpEZRVzX/iajxBJFHBKRVBHZKyK7RWST98FRxc99qmqKqs70cx85RKSCiDwiImu99/mziNwiIhKN/ecTzykikhb6mqo+rKpX+LQ/EZHrRWSRiOwRkTQReVtEWvmxv6ISkftEZEJxtqGqE1W1ZwT7+ktyjObfZGlniSJ+9VHVKkBb4DhgeMDxHDQRKVvArLeBHkBvoCpwCTAYGOlDDCIisfZ/MBK4AbgeqAkcDbwHnFXSOwrzO/BdkPs2B0lV7RFnDyAVOC1k+nHgw5DpLsAs4DdgPnBKyLyawKvABmAH8F7IvLOBed56s4DWefcJ1Af2AjVD5h0HbAXKedP/BJZ6258OHBGyrAJDgJ+B1fm8tx7APuDwPK93BrKApt70TOAR4HtgJzA1T0zhjsFM4CHgO++9NAX+4cW8C1gFXOUtW9lbJhvY7T3qA/cBE7xlGnvv6+/AWu9Y3Bmyv4rAa97xWArcCqQV8Ltt5r3PTmF+/+OB54EPvXj/BxwVMn8ksA74HfgB6Boy7z5gCjDBm38F0An4r3esNgLPAeVD1kkBPgW2A5uBO4Azgf1AhndM5nvLVgde9razHngQSPLmXeYd86e9bT3ovfatN1+8eb96v9MFwLG4LwkZ3v52A+/n/T8Akry4fvGOyQ/k+RuyRzE+c4IOwB5F+KUd+A/SEFgIjPSmGwDbcN/GywCne9N1vPkfAm8ChwDlgJO919t5/6CdvX+6v3v7qZDPPr8ArgyJ5wlgtPf8b8BKoAVQFrgLmBWyrHofOjWBivm8t0eBrwp432vI/QCf6X0QHYv7MP8PuR/chR2DmbgP9BQvxnK4b+tHeR9WJwPpQDtv+VPI88FO/oniJVxSaAP8AbQIfU/eMW/ofQAWlCiuBtYU8vsfj/ug7eTFPxGYHDL/YqCWN+9mYBOQHBJ3hvd7KuPF2x6XWMt672UpcKO3fFXch/7NQLI33TnvMQjZ93vAGO93UheXyHN+Z5cBmcB13r4qcmCiOAP3AV/D+z20AA4Lec8Phvk/uAX3f9DcW7cNUCvo/9VEeQQegD2K8Etz/yC7cd+cFPgcqOHNuw14I8/y03Ef/Ifhvhkfks82XwQeyPPacnITSeg/5RXAF95zwX177eZNfwxcHrKNMrgP3SO8aQVODfPexoV+6OWZNxvvmzruw/7RkHktcd84k8Idg5B17y/kGL8H3OA9P4XIEkXDkPnfAwO856uAM0LmXZF3eyHz7gRmFxLbeGBcyHRvYFmY5XcAbULi/rqQ7d8IvOs9Hwj8VMByfx4Db7oeLkFWDHltIPCl9/wyYG2ebVxGbqI4FViBS1pl8nnP4RLFcuBcP/7f7KHWRxHH/qaqVXEfYscAtb3XjwD6i8hvOQ/gJFySOBzYrqo78tneEcDNedY7HNfMktcU4HgRqQ90w31IfhOynZEh29iOSyYNQtZfF+Z9bfVizc9h3vz8trMGd2ZQm/DHIN8YRKSXiMwWke3e8r3JPaaR2hTyPB3IucCgfp79hXv/2yj4/UeyL0TkZhFZKiI7vfdSnQPfS973frSIfOBdGPE78HDI8ofjmnMicQTud7Ax5LiPwZ1Z5LvvUKr6Ba7Z63lgs4iMFZFqEe77YOI0B8kSRZxT1a9w37ZGeC+tw32brhHyqKyqj3rzaopIjXw2tQ54KM96lVR1Uj77/A2YAVwAXARMUu9rnbedq/Jsp6KqzgrdRJi39BnQWUQOD31RRDrhPgy+CHk5dJlGuCaVrYUcg7/EICIVcE1XI4B6qloD+AiX4AqLNxIbcU1O+cWd1+dAQxHpUJQdiUhX3BnVBbgzxxq49v7QK8byvp8XgWVAM1Wthmvrz1l+Ha5JLj95t7MOd0ZRO+S4V1PVlDDrHLhB1VGq2h7XLHg0rkmp0PUKidMUkyWKxPAMcLqItMV1UvYRkTNEJElEkr3LOxuq6kZc09ALInKIiJQTkW7eNl4CrhaRzt6VQJVF5CwRqVrAPv8PuBTo6z3PMRoYLiIpACJSXUT6R/pGVPUz3Iflf0QkxXsPXXDt8C+q6s8hi18sIi1FpBJwPzBFVbPCHYMCdlseqABsATJFpBcQesnmZqCWiFSP9H3k8RbumBwiIg2AoQUt6L2/F4BJXszlvfgHiMjtEeyrKq4fYAtQVkTuAQr7Vl4V17G9W0SOAa4JmfcBcKiI3OhdtlxVRDp78zYDjXOuGvP+vmYAT4pINREpIyJHicjJEcSNiHT0/v7KAXtwFzVkhezryDCrjwMeEJFm3t9vaxGpFcl+TeEsUSQAVd0CvA7crarrgHNx3wq34L5p3ULu7/oS3DfvZbjO6xu9bcwFrsSd+u/AdUhfFma303BX6GxW1fkhsbwLPAZM9poxFgG9DvIt9QW+BD7B9cVMwF1Jc12e5d7AnU1twnW0Xu/FUNgxOICq7vLWfQv33i/y3l/O/GXAJGCV16SSX3NcOPcDacBq3BnTFNw374JcT24TzG+4JpXzgPcj2Nd03JeBFbjmuH2Eb+oCGIZ7z7twXxjezJnhHZvTgT644/wz0N2b/bb3c5uI/Og9vxSXeJfgjuUUImtKA5fQXvLWW4Nrhss5U34ZaOkd//fyWfcp3O9vBi7pvYzrLDclQHJbDIyJHyIyE9eRGsjd0cUhItfgOroj+qZtTNDsjMIYn4nIYSJyotcU0xx3qem7QcdlTKR8SxQi8oqI/CoiiwqYLyIySkRWisgCEWnnVyzGBKw87uqfXbjO+Km4fghj4oJvTU9eJ+lu4HVVPTaf+b1xbc69cTd5jVTVznmXM8YYEyzfzihU9WvcNfQFOReXRFRVZwM1RCTSTi9jjDFREmRRrgYceDVGmvfaxrwLishgXL0XKleu3P6YY46JSoDGmOLbn5nN8s27gg6j1Kqzezt19+zgJ9WtqlqnKNsIMlHkVzI633YwVR0LjAXo0KGDzp0718+4jDElaNWW3Zz65Ffc16clZxx7aNDhlB6qIEKFjz6gwpefU+WlMWuKuqkgE0UaB96h2hBX0dQYk4AOqVyew6rbrQ2+27EDhg2DI4+EO++Egf3d46UxRd5kkJfHTgMu9a5+6gLs9O7sNMYYUxTvvgstW8Jrr0FGRolt1rczChGZhCtYV1vc6GD34gqGoaqjcbV0euPuAE7HjQdgjDHmYG3eDNddB2+/DW3bwocfQruSu+PAt0ShqgMLmZ8zgI0xxpjiWLfOJYeHHoJbboFy5Up08zYUoTGmUBlZ2azfsbdI667/rWjrmUKsWQPvvw9Dh0KHDrB2LdTypw6iJQpjTKHufHchb81NK9Y2KpS1ikElIjsbXnwRbveKCfftC4cd5luSAEsUxpgIbN+znwY1KjLsjKOLtH6Fskl0P6Zu4Qua8JYvhyuugG+/hTPOgDFjXJLwmSUKY0xEalQqx3nHFTSkh/FdejqcdBJkZcH48XDppSD53Y5W8ixRGGNMLFuxApo1g0qV4I033FVNh0b3xkVrNDTGmFi0b5+7Ya5lS5g40b125plRTxJgZxTGGBN7vvsOLr/c9Un84x9w1lmBhmNnFMYYE0seeAC6dnVnFNOnwyuvwCGHBBqSnVEYEyce+nAJ0+YHUw5tR3oGR9erEsi+Sw2viB9t27q7rB96CKrExjG3RGFMnPjf6u2UEeHko4tUKbrYugW034S3fTv861/QtCncfTf06eMeMcQShTFx5JhDq/Jo39ZBh2FKypQpMGSISxZ33x10NAWyRGGMMdG2caMrvfHOO9C+PcyYAW3aBB1Vgawz2xhjom3DBtdR/dhjMHt2TCcJsDMKY4yJjtRUV8TvuuvcWcS6dYFfzRQpO6Mwxhg/ZWXBqFFw7LHuBrpNm9zrcZIkwM4oEt6bc9Yy65dtQYdhSsCabenUqlw+6DDMwVi61BXxmzXL3VU9Zkwgd1YXlyWKBDfmq1Vs+n0fdatWCDoUU0yHVCrHiU1rBx2GiVR6OnTr5sqCv/46XHxx1Ir4lTRLFKVAjxb1eHbgcUGHYUzpsGwZNG/uivhNnOg6quvVCzqqYrE+CmOMKQl798Jtt0FKSm4Rv5494z5JgJ1RGGNM8X39teuL+Pln9/Pss4OOqETZGYUxxhTHv/8NJ58MmZnw2Wfw0ktQo0bQUZUoSxTGGFMUqu5nhw6uVtPChdCjR7Ax+cSangKyYvMu5qbu8H0/v+/L8H0fxpQqW7e6xNCsGdxzjxsrIuDxIvxmiSIg901bHLX7G+zSWGNKgCq8/bar0bRjB9x7b9ARRY0lioBkZGXT4YhDeH5QO9/3VaeKJQpjimXDBrj2Wpg61TU1ffYZtC49VXwtUQSofNky1KuWHHQYxpjCbNoEX3wBTzwBN94IZUvXR2fperfGGBOpVatg2jSXGNq1g7VrE+5qpkjZVU/GGBMqKwueftoV8bv33twifqU0SYAlCmOMybV4MZx4Itx0E5x6qpuOwyJ+Jc2ankrQll1/sCvCy1H3ZmRRLsnytDExIz3d3TgnAv/3fzBgQNwW8StplihKyK+/76PLI5+TrZGv0725DVZvTOCWLIEWLVwRv8mTXRG/Ova/GcoSRQn5fV8G2Qp/P/4I2h0R2YAk7RrFz8AlxiSc9HTXB/HUUzB+PFxyCZx2WtBRxSRLFCWsQ+Oa9GlTP+gwjDHhzJwJV14JK1fCVVfBOecEHVFMs0ZyY0zpcu+90L27u9P6iy9g9GioXj3oqGKaJQpjTOmQU8SvUye4+WZYsMAlDFMoXxOFiJwpIstFZKWI3J7P/Ooi8r6IzBeRxSLyDz/jMcaUQlu2wEUXwf33u+mzzoIRI1zntYmIb4lCRJKA54FeQEtgoIi0zLPYEGCJqrYBTgGeFBEbPd4YU3yq7jLXFi1gyhQobx8tReXnGUUnYKWqrlLV/cBk4Nw8yyhQVUQEqAJsBzJ9jMkX23b/wVmjvgXssmtjYkJamuugHjQImjaFn36C4cODjipu+ZkoGgDrQqbTvNdCPQe0ADYAC4EbVDU774ZEZLCIzBWRuVu2bPEr3iLb8Ns+/sjMpnGtSpxwVO2gwzHGbNnihid96in47js3jrUpMj8TRX7frfPejnYGMA+oD7QFnhORan9ZSXWsqnZQ1Q51YvhGmLvOaknNynZ6a0wgVq50NZoAjjsO1q1zAwwlJQUbVwLwM1GkAYeHTDfEnTmE+gfwjjorgdXAMT7GZIxJNJmZrnO6VSs3fvXmze71an/5zmmKyM9EMQdoJiJNvA7qAcC0PMusBXoAiEg9oDmwyseYjDGJZOFCOOEEuOUW6NnTFfGrVy/oqBKOb3dmq2qmiAwFpgNJwCuqulhErvbmjwYeAMaLyEJcU9VtqrrVr5iMMQkkPd3dB1GmjKvRdMEFdjWJT3wt4aGqHwEf5XltdMjzDUBPP2MwxiSYRYtc53SlSvDmm66IX227iMRPdmd2Mezcm8Ht/1nA49OXBR2KMYlvzx43TkTr1jBhgnutRw9LElFgRQGLYUHab0yes4761ZNJqV+No+tVDTokYxLT55+7In6rV8O118K5eW/JMn6yRFECRg08jg6NawYdhjGJ6e674cEHoVkz+Oor6NYt6IhKHWt6MsbEpmzv3tsTToBbb4X58y1JBMQShTEmtvz6qxuG9N//dtO9esFjj0HFisHGVYpZojDGxAZV10ndogW8+65Vd40hliiMMcFbtw7OPtsNR9q8uSvid9ttQUdlPNaZfRB27cvgo4UbychyJatW/ro74IiMSRDbtrnifSNHwpAhVp8pxliiOAgfLNjI8HcWHvBaUhmhVpUKAUVkTBxbsQKmTYNhw6BtW3dWUdUuMY9FligOQkaWuwpj+o3dOKRyOQCSyyVRLblckGEZE18yM+HJJ93Y1RUruuamevUsScQw66MogtpVylO3ajJ1qyZbkjDmYMyfD507w+23Q+/esGSJFfGLA3ZGYYyJjvR0V3KjbFk3NGnfvkFHZCJkicIY468FC9xYEZUqwdtvuyJ+Na2SQTyxpidjjD9274YbbnAd1W+84V7r3t2SRByyM4oCZGUrqdv2oCGDt27Z9UdwARkTTz79FAYPhtRUGDoUzjsv6IhMMViiKMCIGct5ceYvf3ldBMqVtRMxYwp0553w8MPuxrlvvoGTTgo6IlNMEScKEamsqnv8DCaW/Ja+n2rJZXnwvFYHvF6vagW70smY/GRnu9HmTjoJhg+He+6B5OSgozIloNBEISInAOOAKkAjEWkDXKWq1/odXNCSyyVxTpv6QYdhTGzbtMk1L7VsCfff74r49eoVdFSmBEXShvI0cAawDUBV5wNW69eY0k4Vxo93CeKDD6BataAjMj6JqOlJVdfJgYOWZ/kTjjEmLqxZ4zqrZ8xwTU3jxrk+CZOQIjmjWOc1P6mIlBeRYcBSn+MyxsSy336DOXPguefcqHOWJBJaJGcUVwMjgQZAGjADSPj+CWNMHsuXuyJ+t9zibppbuxaqVAk6KhMFkZxRNFfVQapaT1XrqurFQAu/AzPGxIiMDHjkEZccHn3UjUAHliRKkUgSxbMRvmaMSTQ//eSK+N1xB/Tp44r41a0bdFQmygpsehKR44ETgDoiclPIrGqAjSpiTKJLT4fTT4dy5eA//4Hzzw86IhOQcH0U5XH3TpQFQgvF/w708zMoY0yAfvrJ1WeqVMlVeW3TBg45JOioTIAKTBSq+hXwlYiMV9U1UYzJGBOEXbvcHdXPPw+vvQaXXgqnnBJ0VCYGRHLVU7qIPAGkAH/ej6+qp/oWlTEmuj75BK66yg1HesMN1sxkDhBJZ/ZEYBnQBPg3kArM8TEmY0w0DR/uSm5UrgzffQfPPGNXNJkDRHJGUUtVXxaRG0Kao77yO7AgzFi8ianzNwCwIO23gKMxxmdZWZCU5JqXypaFu+6CChWCjsrEoEgSRYb3c6OInAVsABr6F1Jw/u/7tcz6ZRuHH1KR8kll6NasTtAhGVPyNm6EIUMgJQUeeADOOMM9jClAJIniQRGpDtyMu3+iGnCjr1EFqMWhVZk61OrnmwSUU8Tvpptg3z4bJ8JErNBEoaofeE93At0BROREP4MyxpSw1FS48kr47DPo2tUV8Tv66KCjMnEi3A13ScAFuBpPn6jqIhE5G7gDqAgcF50QjTHFtnMn/PgjvPCCu7qpjI3SaCIX7q/lZeAKoBYwSkReBUYAj6tqRElCRM4UkeUislJEbi9gmVNEZJ6ILE7UTnJjArFkiavNBLlF/K65xpKEOWjhmp46AK1VNVtEkoGtQFNV3RTJhr0zkueB03FVZ+eIyDRVXRKyTA3gBeBMVV0rIlZExpji2r8fHn/cdVRXrQr//Kerz1S5ctCRmTgV7qvFflXNBlDVfcCKSJOEpxOwUlVXqep+YDJwbp5lLgLeUdW13n5+PYjtG2PymjsXOnaEu+92N81ZET9TAsKdURwjIgu85wIc5U0LoKraupBtNwDWhUynAZ3zLHM0UE5EZuLqSY1U1dfzbkhEBgODARo1alTIbo0ppfbscZe5JifD1KlwzjlBR2QSRLhEUdwxJySf1zSf/bcHeuA6yP8rIrNVdcUBK6mOBcYCdOjQIe82jCndfvzRFfGrXBnefRdat4YaNYKOyiSQApueVHVNuEcE204DDg+Zboi7WS/vMp+o6h5V3Qp8DbQ52DdhTKn0++9w7bXQvj1MmOBe69bNkoQpcX5e/jAHaCYiTUSkPDAAmJZnmalAVxEpKyKVcE1TNh63MYX56CN3Z/WYMe4Gur59g47IJLBI7swuElXNFJGhwHTcQEevqOpiEbnamz9aVZeKyCfAAiAbGKeqi/yKyZiEcNtt7qqmli3deBGd83b9GVOyIkoUIlIRaKSqyw9m46r6EfBRntdG55l+AnjiYLZrTKmjCtnZrohfjx6uw/qOO6yIn4mKQpueRKQPMA/4xJtuKyJ5m5Di2t79Wfy8eRd7/sgMOhRj/mr9evjb3+Dee910z57w739bkjBRE0kfxX24eyJ+A1DVeUBj/0KKvmsm/sDpT3/NnNQdVChnw4GbGKEKL73kmphmzIDatYOOyJRSkTQ9ZarqTpH8rnZNDL+lZ3DMoVUZempTWjWoHnQ4xsDq1XD55fDll268iJdegqZNg47KlFKRJIpFInIRkCQizYDrgVn+hhV9daslc3br+kGHYYyzezcsWOCuarriCqvPZAIVyV/fdbjxsv8A/g9Xbjxhx6MwJjCLFsHDD7vnrVq5In6DB1uSMIGL5C+wuareqaodvcddXu0nY0xJ2L/fdU63awdPPw2/eiXPKlUKNi5jPJEkiqdEZJmIPCAiKb5HZExpMmeOu7P6vvugf38r4mdiUiQj3HUXkUNxgxiNFZFqwJuq+qDv0RmTyPbsgTPPhIoVYdo06NMn6IiMyVdEjZ+quklVRwFX4+6puMfXqKLozncXMm/db/lWMDTGF3PnupvnKld2VV4XL7YkYWJaJDfctRCR+0RkEfAc7oqnhr5HFiVzU3cAcFW3IwOOxCS8nTvdMKQdO+YW8TvpJKhul2Sb2BbJ5bGvApOAnqqat/pr3BOBM1LqcUJTu5nJ+Oj99+Hqq2HTJhg2DPr1CzoiYyIWSR9Fl2gEYkzCuuUWGDHCXfL63nvujMKYOFJgohCRt1T1AhFZyIEDDkU6wp0xpZcqZGVB2bKuNlO1aq7qa/nyQUdmzEELd0Zxg/fz7GgEYkzCSEuDa65xI8099BCcfrp7GBOnwo1wt9F7em0+o9tdG53wjIkj2dmu5EbLlvDFF3DooUFHZEyJiOTy2Py+CvUq6UCC8MZ/U1m2aVfQYZhEsGoVnHqq67Du1AkWLoTrrgs6KmNKRLg+imtwZw5HisiCkFlVge/8DiwaXvpmNQDdjq4TcCQm7u3Z4+6qHjcO/vlPdzmdMQkiXB/F/wEfA48At4e8vktVt/saVZSUETi3bX0GdT4i6FBMPFq40N0wd9dd7oqmNWvcXdbGJJhwTU+qqqnAEGBXyAMRqel/aMbEqD/+gHvucUX8Ro3KLeJnScIkqMLOKM4GfsBdHht6Lq2A3cpsSp/Zs92AQkuWwCWXuGqvtWoFHZUxviowUajq2d7PJtELx5gYtmcPnHWWq9H00UfQKyGu6TCmUJHUejpRRCp7zy8WkadEpJH/oRkTI/73v9wifu+/74r4WZIwpUgkl8e+CKSLSBvgVmAN8IavURkTC377zQ1D2qVLbhG/E06AqlWDjcuYKIskUWSqqgLnAiNVdSTuEtm4tzcji/JJNsykycd777kb58aPd6U3+vcPOiJjAhNJ9dhdIjIcuAToKiJJQDl/w/Lf3v1ZbP79DxrVtOEmTR433eQ6qdu0cU1N7dsHHZExgYokUVwIXAT8U1U3ef0TT/gblv9St+0BoHHtygFHYmJCaBG/3r3dlUy33grl4v47kTHFVmi7i6puAiYC1UXkbGCfqr7ue2Q+W5OTKGpZoij11q51VzPde6+bPu00uPNOSxLGeCK56ukC4HugP27c7P+JSNyPurJ6azoAR9S2pqdSKzsbXngBUlLgq6+gfv2gIzImJkXS9HQn0FFVfwUQkTrAZ8AUPwPzW+rWPdSqXJ5qyfatsVRaudLVZPrmG1cCfOxYaNw46KiMiUmRJIoyOUnCs43IrpaKaanb9lj/RGm2bx+sWAGvvgp//7sV8TMmjEgSxSciMh03bja4zu2P/AspOtJ27KVTEytZVarMm+eK+N17Lxx7LKSmQnJy0FEZE/Mi6cy+BRgDtAbaAGNV9Ta/A/NbVrbaPRSlxb59rnO6Qwd48cXcIn6WJIyJSLjxKJoBI4CjgIXAMFVdH63AjCkRs2a5In7Llrkmpqeegpp2JmnMwQj3lfoV4AOgL66C7LNRiciYkrJnD/TpA+np8Mkn7i5rSxLGHLRwfRRVVfUl7/lyEfkxGgEZU2z//S907uyK+H3wgeuPsPpMxhRZuDOKZBE5TkTaiUg7oGKe6UKJyJkislxEVorI7WGW6ygiWYlwf4YJ0I4d7pLXE06AN7y6lccfb0nCmGIKd0axEXgqZHpTyLQCp4bbsFcT6nngdCANmCMi01R1ST7LPQZMP7jQjQnxzjswZAhs2QLDh8OFFwYdkTEJI9zARd2Lue1OwEpVXQUgIpNxFWiX5FnuOuA/QMdi7i8iF4/7H4s27GTn3gy7dD5R/Otf8Mwz0LatG1DouOOCjsiYhBLJfRRF1QBYFzKdBnQOXUBEGgDn4c5OCkwUIjIYGAzQqFHxxkyak7qdpnWr8Le2DejXvmGxtmUCFFrE7+yzoW5dGDbM6jMZ4wM/byTI7/u65pl+BrhNVbPCbUhVx6pqB1XtUKdOnWIHdlKz2tx3TgrHNqhe7G2ZAKSmwplnwt13u+kePVxzkyUJY3zhZ6JIAw4PmW4IbMizTAdgsoikAv2AF0Tkbz7GZOJZdjY8+6y7imnWLDjiiKAjMqZUKLTpSUQEGAQcqar3e+NRHKqq3xey6hygmYg0AdYDA3DjWvxJVZuE7Gc88IGqvndwb8GUCj//DP/4B3z3nTubGD3aEoUxURLJGcULwPHAQG96F+5qprBUNRMYiruaaSnwlqouFpGrReTqIsZrSqv9++GXX+D1112HtSUJY6Imks7szqraTkR+AlDVHSJSPpKNq+pH5CkgqKqjC1j2ski2aUqRn35yRfzuu8+NGZGaChUqBB2VMaVOJGcUGd69Dgp/jkeR7WtUpnTbt891Ti9qIEwAABewSURBVHfsCGPGuHsjwJKEMQGJJFGMAt4F6orIQ8C3wMO+RmVKr2+/hTZt4NFH4dJLYckSKIEr3YwxRVdo05OqThSRH4AeuEte/6aqS32PzJQ+u3fDuedCtWowY4Ybec4YE7hIrnpqBKQD74e+pqpr/QzMlCLffuvqM1WpAh9+6C5/rVIl6KiMMZ5Imp4+xJUb/xD4HFgFfOxnUKaU2LbNNS917ZpbxK9LF0sSxsSYSJqeWoVOe5Vjr/ItIpP4VGHKFBg6FLZvd3dYDxgQdFTGmAIcdK0nVf1RRKJSwM8kqH/9C0aOhPbtXV9EmzZBR2SMCSOSPoqbQibLAO2ALb5FZBKTKmRmunpM55wD9evDTTe5on7GmJgWSR9F1ZBHBVxfxbl+BmUSzOrV0LNnbhG/U0+FW2+1JGFMnAj7n+rdaFdFVW+JUjwmkWRlwXPPwR13QFIS9O8fdETGmCIoMFGISFlVzYx02FNjDrBiBVx2mRu/ulcvd4f14YcXupoxJvaEO6P4HtcfMU9EpgFvA3tyZqrqOz7HZuJZZiasWQMTJsBFF2HDCRoTvyJpJK4JbMONQqe4u7MVsERhDjR3rivi98AD0LIlrFpl9ZmMSQDhEkVd74qnReQmiBx5R6ozpdnevXDvvfDkk3DooXD99a4+kyUJYxJCuESRBFQhsiFNY9rm3/exbfd+wF2laUrQV1/BFVfAypVw5ZXw+ONQo0bQURljSlC4RLFRVe+PWiQ+2bs/i66Pf8n+zNzK6MllkwKMKIHs3g3nn+8Sw+efu8tejTEJJ1yiSIjex30ZWezPzObCDofT/Zi6lBE4/qhaQYcV3775Bk480dVk+vhjN6hQ5cpBR2WM8Um4G+56RC2KKGhxWFXOPPZQeqYcStXkckGHE5+2boWLL4Zu3XKL+HXqZEnCmARX4BmFqm6PZiAmhqnCW2/BddfBjh2u49qK+BlTalgNBVO4G26AZ591Q5N+/jm0alX4OsaYhGGJwuRPFTIyoHx5OO88OOIIuPFGV4rDGFOqRFIU0JQ2v/wCPXrAXXe56e7d4eabLUkYU0oldKLYuz+LM575GgCxEhKFy8qCp55yTUs//ADNmwcdkTEmBiR009OO9P38uusPalUuzxkphwYdTmxbtgz+/nf4/nvo0wdefBEaNAg6KmNMDEjoRJHj1jObc2j15KDDiG3Z2bBhA0yaBBdeaEX8jDF/KhWJwhTg++9dEb+HHnJF/H75xXVeG2NMiITuozAFSE+HYcPg+OPhtddgizeyrSUJY0w+LFGUNl9+6Tqrn3zSFfFbvNhVejXGmAJY01Npsnu3G460Rg2XME45JeiIjDFxwM4oSoOZM11ndU4RvwULLEkYYyJmiSKRbdkCAwe6G+YmTHCvdewIlSoFG5cxJq5Y01MiUnWXuV5/Peza5YYmtSJ+xpgiskSRiK67Dp5/Hrp0gZdfdpe+GmNMEVmiSBTZ2ZCZ6S5x7dcPmjZ1CcPqMxljisnXPgoROVNElovIShG5PZ/5g0RkgfeYJSJt/IwnYf38sxuG9M473fQpp1ilV2NMifEtUYhIEvA80AtoCQwUkbxtIKuBk1W1NfAAMNaveBJSZiaMGAGtW8O8edCiRdARGWMSkJ9NT52Alaq6CkBEJgPnAktyFlDVWSHLzwYa+hhPYlm6FC69FObOhXPPhRdegPr1g47KGJOA/Gx6agCsC5lO814ryOXAx/nNEJHBIjJXROZuySk3YWDzZnjzTXj3XUsSxhjf+Jko8is/qvkuKNIdlyhuy2++qo5V1Q6q2qFOaS43MXs2DB/unrdo4Yr4XXCBVXo1xvjKz0SRBhweMt0Q2JB3IRFpDYwDzlXVbT7GE7/27IF//QtOOAEmTswt4leuXLBxGWNKBT8TxRygmYg0EZHywABgWugCItIIeAe4RFVX+BhL/PrsMzj2WHjmGbj2WiviZ4yJOt86s1U1U0SGAtOBJOAVVV0sIld780cD9wC1gBe8oUozVbWDXzHFnd273R3VNWvC119D165BR2SMKYV8veFOVT8CPsrz2uiQ51cAV/gZQ1z64gs4+WRXxG/6dHdndcWKQUdljCmlEqoooKqyastuFq3fyaL1O1m+eVfQIR2czZtd53SPHrlF/Nq3tyRhjAlUQpXw+Gndb5z/wqy/vJ5cLsbvUFZ1ieHGG11z00MPwUUXBR2VMcYACZYolm78HYAn+rWmekV3RVD5smU44ajaQYZVuCFD4MUX3dCkL79sd1gbY2JKQiWK1K17KF+2DH3bNaRMmRi/tyA7GzIyoEIFuPBClxyuvdbqMxljYk5C9VGkbkvniJqVYj9JLF/uOqtzividfLJVejXGxKzEShRb99C4duWgwyhYRgY8+ii0aQOLFkGrVkFHZIwxhUqYpqfsbGXN9nS6H1M36FDyt3gxXHIJ/PQTnH++G1jo0EODjsoYYwqVMIli4+/72J+ZzRG1YnQ86KQk2L4dpkyBvn2DjsYYYyIW14nif6u2cc3EH8nIzCZLXb3BJrViqOlp1iyYOhUeewyOOQZWroSycX3IjTGlUFx/aq3cspvte/YzoOPhVCpflirJZenQuGbQYbl7Ie64A557Dho1gltugdq1LUkYY+JSQnxy3XT60dStlhx0GM6MGTB4MKxdC0OHwsMPu1IcxhgTpxIiUcSM3bth0CCoVQu++QZOPDHoiIwxptgS6vLYwHz6KWRluTOHGTPc+NWWJIwxCcISRXFs3OiuYOrZ0w0oBHDccZAcI81gxhhTAixRFIUqjB/vyn9/+KG7ic6K+BljElTc9VGs2ZbOFa/NASBtx95ggrjmGhgzBk46CcaNg+bNg4nDmBKQkZFBWloa+/btCzoUUwKSk5Np2LAh5UpwqOS4SxS/78tg4073B51URjj1mLocUrm8/zsOLeJ30UXQujVcfTWUsZMyE9/S0tKoWrUqjRs3xhtp0sQpVWXbtm2kpaXRpEmTEttu3CWKMiJ8eH2UhwRduhSuuAK6dIEnn4Ru3dzDmASwb98+SxIJQkSoVasWW7ZsKdHt2tfhcDIy3H0QbdvCsmWuo9qYBGRJInH48buMuzOKqFm8GC6+2F3q2r8/PPss1KsXdFTGGBN1dkZRkLJlYedOeOcdeOstSxLG+EhEuOSSS/6czszMpE6dOpx99tkAjB8/nqFDh/5lvcaNG9OqVSvatGlDz5492bRpU77b37JlC+XKlWPMmDEHvF4lT9WEvPt5/fXXOfbYY0lJSaFly5aMGDGiyO8xxyeffELz5s1p2rQpjz76aL7L7Ny5kz59+tCmTRtSUlJ49dVX/5z39NNPk5KSwrHHHsvAgQOjchGCJYpQ33wDw4a5582bw4oVcN55wcZkTClQuXJlFi1axN697krGTz/9lAYNGkS07pdffsn8+fPp0KEDDz/8cL7LvP3223Tp0oVJkyZFHNPHH3/MM888w4wZM1i8eDE//vgj1atXj3j9/GRlZTFkyBA+/vhjlixZwqRJk1iyZMlflnv++edp2bIl8+fPZ+bMmdx8883s37+f9evXM2rUKObOncuiRYvIyspi8uTJxYopEtb0BLBrF9x+O7zwAjRp4p5bET9TCv37/cUs2fB7iW6zZf1q3NsnpdDlevXqxYcffki/fv2YNGkSAwcO5Jtvvol4P926dWPUqFH5zps0aRJPPvkkF110EevXr48oCT3yyCOMGDGC+vXrA+6y0yuvvDLiePLz/fff07RpU4488kgABgwYwNSpU2nZsuUBy4kIu3btQlXZvXs3NWvWpKz3eZSZmcnevXspV64c6enpf8bnJzuj+PhjSEmBF1+EG2+EhQtdkjDGRNWAAQOYPHky+/btY8GCBXTu3Pmg1v/ggw9olc+okevWrWPTpk106tSJCy64gDfffDOi7S1atIj27dsXutzEiRNp27btXx79+vX7y7Lr16/n8MMP/3O6YcOGrF+//i/LDR06lKVLl1K/fn1atWrFyJEjKVOmDA0aNGDYsGE0atSIww47jOrVq9OzZ8+I3k9xlO6vzLt2waWXQt26buyILl2CjsiYQEXyzd8vrVu3JjU1lUmTJtG7d++I1+vevTtJSUm0bt2aBx988C/zJ0+ezAUXXAC4ZHT55Zdz0003Fbi9g71qaNCgQQwaNCiiZdUbN6ew/U2fPp22bdvyxRdf8Msvv3D66afTtWtXsrKymDp1KqtXr6ZGjRr079+fCRMmcPHFFx9UzAer9CUKVZg+HU4/HapWhc8+c4MKVagQdGTGlHrnnHMOw4YNY+bMmWzbti2idb788ktqh2kFmDRpEps3b2aiV49tw4YN/PzzzzRr1oyKFSuyf/9+ypd3N+1u3779z22lpKTwww8/cOqpp4bd/8SJE3niiSf+8nrTpk2ZMmXKAa81bNiQdevW/TmdlpaWb9PRq6++yu23346I0LRpU5o0acKyZctYs2YNTZo0oU6dOgCcf/75zJo1y/dEUbqanjZudONV9+qVW8SvTRtLEsbEiH/+85/cc889+TYhFcXy5cvZs2cP69evJzU1ldTUVIYPH/5nB/DJJ5/MhAkTANi7dy9vvfUW3bt3B2D48OHceuutf15J9ccff+TbBzJo0CDmzZv3l0feJAHQsWNHfv75Z1avXs3+/fuZPHky55xzzl+Wa9SoEZ9//jkAmzdvZvny5Rx55JE0atSI2bNnk56ejqry+eef06JFixI5VuGUjkShCq+8Ai1awCefwOOPWxE/Y2JQw4YNueGGG/KdN378eBo2bPjnIy0trdDtTZo0ifPyXLnYt2/fP69+GjlyJO+88w5t27alS5cu9O/fn25e1YXevXszZMgQTjvtNFJSUmjfvj2ZmZnFen9ly5blueee44wzzqBFixZccMEFpKS45r7Ro0czevRoAO6++25mzZpFq1at6NGjB4899hi1a9emc+fO9OvXj3bt2tGqVSuys7MZPHhwsWKKhOTXZhbLKtY/WvduWHFwK111FYwd68pujBsHzZr5E5wxcWjp0qVR+VZqoie/36mI/KCqHYqyvcTto8jKciU4kpPdHdbHHeeGKLUifsYYc1AS81Nz8WI3wtwdd7jprl2t0qsxxhRRYn1y7t8PDzzgzh5WroSOHYOOyJi4EG9N0KZgfvwuE6fpaeFCGDTI/RwwAEaNAu8SMmNMwZKTk9m2bRu1atWyKrJxLmc8iuQSHo45cRJF+fKQng5Tp0I+l5sZY/KXcwVRSY9hYIKRM8JdSYrvq56++gqmTXODCYHrwE5KCi44Y4yJUcW56snXPgoROVNElovIShG5PZ/5IiKjvPkLRKRdRBv+/Xc3bvUpp8B778HWre51SxLGGFPifEsUIpIEPA/0AloCA0WkZZ7FegHNvMdg4MXCtlt13x5XxG/sWLjpJiviZ4wxPvPzjKITsFJVV6nqfmAycG6eZc4FXldnNlBDRA4Lt9H6OzdB9equiN+TT0KlSv5Eb4wxBvC3M7sBsC5kOg3IWzc4v2UaABtDFxKRwbgzDoA/ZPHiRVbpFYDawNagg4gRdixy2bHIZcciV/OiruhnosjvOru8PeeRLIOqjgXGAojI3KJ2yCQaOxa57FjksmORy45FLhGZW9R1/Wx6SgMOD5luCGwowjLGGGMC5GeimAM0E5EmIlIeGABMy7PMNOBS7+qnLsBOVd2Yd0PGGGOC41vTk6pmishQYDqQBLyiqotF5Gpv/mjgI6A3sBJIB/4RwabH+hRyPLJjkcuORS47FrnsWOQq8rGIuxvujDHGRFdiFQU0xhhT4ixRGGOMCStmE4Vv5T/iUATHYpB3DBaIyCwRaRNEnNFQ2LEIWa6jiGSJSL9oxhdNkRwLETlFROaJyGIR+SraMUZLBP8j1UXkfRGZ7x2LSPpD446IvCIiv4rIogLmF+1zU1Vj7oHr/P4FOBIoD8wHWuZZpjfwMe5ejC7A/4KOO8BjcQJwiPe8V2k+FiHLfYG7WKJf0HEH+HdRA1gCNPKm6wYdd4DH4g7gMe95HWA7UD7o2H04Ft2AdsCiAuYX6XMzVs8ofCn/EacKPRaqOktVd3iTs3H3oySiSP4uAK4D/gP8Gs3goiySY3ER8I6qrgVQ1UQ9HpEcCwWqihtwowouUWRGN0z/qerXuPdWkCJ9bsZqoiiotMfBLpMIDvZ9Xo77xpCICj0WItIAOA8YHcW4ghDJ38XRwCEiMlNEfhCRS6MWXXRFciyeA1rgbuhdCNygqtnRCS+mFOlzM1YHLiqx8h8JIOL3KSLdcYniJF8jCk4kx+IZ4DZVzUrw0doiORZlgfZAD6Ai8F8Rma2qK/wOLsoiORZnAPOAU4GjgE9F5BtV/d3v4GJMkT43YzVRWPmPXBG9TxFpDYwDeqnqtijFFm2RHIsOwGQvSdQGeotIpqq+F50QoybS/5GtqroH2CMiXwNtgERLFJEci38Aj6prqF8pIquBY4DvoxNizCjS52asNj1Z+Y9chR4LEWkEvANckoDfFkMVeixUtYmqNlbVxsAU4NoETBIQ2f/IVKCriJQVkUq46s1LoxxnNERyLNbizqwQkXq4SqqrohplbCjS52ZMnlGof+U/4k6Ex+IeoBbwgvdNOlMTsGJmhMeiVIjkWKjqUhH5BFgAZAPjVDXfyybjWYR/Fw8A40VkIa755TZVTbjy4yIyCTgFqC0iacC9QDko3uemlfAwxhgTVqw2PRljjIkRliiMMcaEZYnCGGNMWJYojDHGhGWJwhhjTFiWKExM8iq/zgt5NA6z7O4S2N94EVnt7etHETm+CNsYJyItved35Jk3q7gxetvJOS6LvGqoNQpZvq2I9C6JfZvSyy6PNTFJRHarapWSXjbMNsYDH6jqFBHpCYxQ1dbF2F6xYypsuyLyGrBCVR8Ks/xlQAdVHVrSsZjSw84oTFwQkSoi8rn3bX+hiPylaqyIHCYiX4d84+7qvd5TRP7rrfu2iBT2Af410NRb9yZvW4tE5Ebvtcoi8qE3tsEiEbnQe32miHQQkUeBil4cE715u72fb4Z+w/fOZPqKSJKIPCEic8SNE3BVBIflv3gF3USkk7ixSH7yfjb37lK+H7jQi+VCL/ZXvP38lN9xNOYvgq6fbg975PcAsnBF3OYB7+KqCFTz5tXG3Vmac0a82/t5M3Cn9zwJqOot+zVQ2Xv9NuCefPY3Hm/sCqA/8D9cQb2FQGVcaerFwHFAX+ClkHWrez9n4r69/xlTyDI5MZ4HvOY9L4+r5FkRGAzc5b1eAZgLNMknzt0h7+9t4ExvuhpQ1nt+GvAf7/llwHMh6z8MXOw9r4Gr+1Q56N+3PWL7EZMlPIwB9qpq25wJESkHPCwi3XDlKBoA9YBNIevMAV7xln1PVeeJyMlAS+A7r7xJedw38fw8ISJ3AVtwVXh7AO+qK6qHiLwDdAU+AUaIyGO45qpvDuJ9fQyMEpEKwJnA16q612vuai25I/JVB5oBq/OsX1FE5gGNgR+AT0OWf01EmuGqgZYrYP89gXNEZJg3nQw0IjFrQJkSYonCxItBuJHJ2qtqhoik4j7k/qSqX3uJ5CzgDRF5AtgBfKqqAyPYxy2qOiVnQkROy28hVV0hIu1xNXMeEZEZqnp/JG9CVfeJyExc2esLgUk5uwOuU9XphWxir6q2FZHqwAfAEGAUrpbRl6p6ntfxP7OA9QXoq6rLI4nXGLA+ChM/qgO/ekmiO3BE3gVE5AhvmZeAl3FDQs4GThSRnD6HSiJydIT7/Br4m7dOZVyz0TciUh9IV9UJwAhvP3lleGc2+ZmMK8bWFVfIDu/nNTnriMjR3j7zpao7geuBYd461YH13uzLQhbdhWuCyzEduE680ysROa6gfRiTwxKFiRcTgQ4iMhd3drEsn2VOAeaJyE+4foSRqroF98E5SUQW4BLHMZHsUFV/xPVdfI/rsxinqj8BrYDvvSagO4EH81l9LLAgpzM7jxm4sY0/Uzd0J7ixRJYAP4rIImAMhZzxe7HMx5XVfhx3dvMdrv8ix5dAy5zObNyZRzkvtkXetDFh2eWxxhhjwrIzCmOMMWFZojDGGBOWJQpjjDFhWaIwxhgTliUKY4wxYVmiMMYYE5YlCmOMMWH9P+bsTN2ZS2N1AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fpr_test, tpr_test, threshold_test = roc_curve(validation_target, model.predict_proba(validation_features)[:,1])\n",
    "roc_auc_test = auc(fpr_test, tpr_test)\n",
    "\n",
    "# image drawing\n",
    "plt.figure()\n",
    "plt.title('Receiver Operating Characteristic ')\n",
    "plt.plot(fpr_test, tpr_test, label = 'MLP AUC = %0.2f' % roc_auc_test)\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLPClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter_space = {\n",
    "    'max_iter' : [50,100],\n",
    "    'hidden_layer_sizes': [(50,50,50), (50,100,50), (100,),(50,30,20),],\n",
    "    'activation': ['tanh', 'relu'],\n",
    "    'solver': ['sgd', 'adam'],\n",
    "    'alpha': [0.0001,0.01,0.1, 0.05],\n",
    "    'learning_rate': ['constant','adaptive'],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 256 candidates, totalling 768 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:   52.8s\n",
      "[Parallel(n_jobs=-1)]: Done 154 tasks      | elapsed:  4.5min\n",
      "[Parallel(n_jobs=-1)]: Done 357 tasks      | elapsed: 10.7min\n",
      "[Parallel(n_jobs=-1)]: Done 640 tasks      | elapsed: 19.5min\n",
      "[Parallel(n_jobs=-1)]: Done 768 out of 768 | elapsed: 23.5min finished\n",
      "C:\\Users\\DELL\\Anaconda3\\envs\\deeplearning_env\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:585: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 256 candidates, totalling 768 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:   51.9s\n",
      "[Parallel(n_jobs=-1)]: Done 154 tasks      | elapsed:  4.4min\n",
      "[Parallel(n_jobs=-1)]: Done 357 tasks      | elapsed: 10.5min\n",
      "[Parallel(n_jobs=-1)]: Done 640 tasks      | elapsed: 18.7min\n",
      "[Parallel(n_jobs=-1)]: Done 768 out of 768 | elapsed: 22.6min finished\n",
      "C:\\Users\\DELL\\Anaconda3\\envs\\deeplearning_env\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:585: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 256 candidates, totalling 768 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:   49.3s\n",
      "[Parallel(n_jobs=-1)]: Done 154 tasks      | elapsed:  4.3min\n",
      "[Parallel(n_jobs=-1)]: Done 357 tasks      | elapsed: 10.4min\n",
      "[Parallel(n_jobs=-1)]: Done 640 tasks      | elapsed: 18.6min\n",
      "[Parallel(n_jobs=-1)]: Done 768 out of 768 | elapsed: 22.6min finished\n",
      "C:\\Users\\DELL\\Anaconda3\\envs\\deeplearning_env\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:585: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 256 candidates, totalling 768 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:   48.6s\n",
      "[Parallel(n_jobs=-1)]: Done 154 tasks      | elapsed:  4.3min\n",
      "[Parallel(n_jobs=-1)]: Done 357 tasks      | elapsed: 10.4min\n",
      "[Parallel(n_jobs=-1)]: Done 640 tasks      | elapsed: 19.3min\n",
      "[Parallel(n_jobs=-1)]: Done 768 out of 768 | elapsed: 23.4min finished\n",
      "C:\\Users\\DELL\\Anaconda3\\envs\\deeplearning_env\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:585: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 256 candidates, totalling 768 fits\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-132-f60abf6584b1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'timeit'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'from sklearn.model_selection import GridSearchCV\\n\\n\\nclf = GridSearchCV(mlp, parameter_space, n_jobs=-1, cv=3, verbose = 2)\\nclf.fit(train_features,train_target)\\n'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\deeplearning_env\\lib\\site-packages\\IPython\\core\\interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[1;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[0;32m   2379\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2380\u001b[0m                 \u001b[0margs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2381\u001b[1;33m                 \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2382\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2383\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<decorator-gen-61>\u001b[0m in \u001b[0;36mtimeit\u001b[1;34m(self, line, cell, local_ns)\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\deeplearning_env\\lib\\site-packages\\IPython\\core\\magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(f, *a, **k)\u001b[0m\n\u001b[0;32m    185\u001b[0m     \u001b[1;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    186\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 187\u001b[1;33m         \u001b[0mcall\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    188\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    189\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\deeplearning_env\\lib\\site-packages\\IPython\\core\\magics\\execution.py\u001b[0m in \u001b[0;36mtimeit\u001b[1;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[0;32m   1171\u001b[0m                     \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1172\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1173\u001b[1;33m         \u001b[0mall_runs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtimer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnumber\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1174\u001b[0m         \u001b[0mbest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_runs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mnumber\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1175\u001b[0m         \u001b[0mworst\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_runs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mnumber\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\deeplearning_env\\lib\\timeit.py\u001b[0m in \u001b[0;36mrepeat\u001b[1;34m(self, repeat, number)\u001b[0m\n\u001b[0;32m    202\u001b[0m         \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    203\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 204\u001b[1;33m             \u001b[0mt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtimeit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnumber\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    205\u001b[0m             \u001b[0mr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    206\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\deeplearning_env\\lib\\site-packages\\IPython\\core\\magics\\execution.py\u001b[0m in \u001b[0;36mtimeit\u001b[1;34m(self, number)\u001b[0m\n\u001b[0;32m    167\u001b[0m         \u001b[0mgc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdisable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    168\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 169\u001b[1;33m             \u001b[0mtiming\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minner\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mit\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtimer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    170\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    171\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mgcold\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<magic-timeit>\u001b[0m in \u001b[0;36minner\u001b[1;34m(_it, _timer)\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\deeplearning_env\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     71\u001b[0m                           FutureWarning)\n\u001b[0;32m     72\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 73\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     74\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\deeplearning_env\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    734\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    735\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 736\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    737\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    738\u001b[0m         \u001b[1;31m# For multi-metric evaluation, store the best_index_, best_params_ and\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\deeplearning_env\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1186\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1187\u001b[0m         \u001b[1;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1188\u001b[1;33m         \u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1189\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\deeplearning_env\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[1;34m(candidate_params)\u001b[0m\n\u001b[0;32m    713\u001b[0m                                \u001b[1;32mfor\u001b[0m \u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    714\u001b[0m                                in product(candidate_params,\n\u001b[1;32m--> 715\u001b[1;33m                                           cv.split(X, y, groups)))\n\u001b[0m\u001b[0;32m    716\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    717\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\deeplearning_env\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1040\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1042\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1043\u001b[0m             \u001b[1;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1044\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\deeplearning_env\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    919\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    920\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'supports_timeout'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 921\u001b[1;33m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    922\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    923\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\deeplearning_env\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[1;34m(future, timeout)\u001b[0m\n\u001b[0;32m    540\u001b[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001b[0;32m    541\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 542\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    543\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mCfTimeoutError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\deeplearning_env\\lib\\concurrent\\futures\\_base.py\u001b[0m in \u001b[0;36mresult\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    428\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    429\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 430\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    431\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    432\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mCANCELLED\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mCANCELLED_AND_NOTIFIED\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\deeplearning_env\\lib\\threading.py\u001b[0m in \u001b[0;36mwait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    294\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m    \u001b[1;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    295\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 296\u001b[1;33m                 \u001b[0mwaiter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    297\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    298\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "clf = GridSearchCV(mlp, parameter_space, n_jobs=-1, cv=3, verbose = 2)\n",
    "clf.fit(train_features,train_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'activation': 'relu',\n",
       " 'alpha': 0.0001,\n",
       " 'hidden_layer_sizes': (50, 100, 50),\n",
       " 'learning_rate': 'adaptive',\n",
       " 'solver': 'adam'}"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature_1</th>\n",
       "      <th>feature_2</th>\n",
       "      <th>feature_3</th>\n",
       "      <th>feature_4</th>\n",
       "      <th>feature_5</th>\n",
       "      <th>feature_6</th>\n",
       "      <th>feature_7</th>\n",
       "      <th>feature_8</th>\n",
       "      <th>feature_9</th>\n",
       "      <th>feature_10</th>\n",
       "      <th>...</th>\n",
       "      <th>feature_1549</th>\n",
       "      <th>feature_1550</th>\n",
       "      <th>feature_1551</th>\n",
       "      <th>feature_1552</th>\n",
       "      <th>feature_1553</th>\n",
       "      <th>feature_1554</th>\n",
       "      <th>feature_1555</th>\n",
       "      <th>feature_1556</th>\n",
       "      <th>feature_1557</th>\n",
       "      <th>feature_1558</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>60.0</td>\n",
       "      <td>468.0</td>\n",
       "      <td>7.8000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>108.0</td>\n",
       "      <td>179.0</td>\n",
       "      <td>1.6574</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>60.0</td>\n",
       "      <td>468.0</td>\n",
       "      <td>7.8000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>60.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>2.0000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>751</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>752</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>753</th>\n",
       "      <td>12.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>8.4166</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>754</th>\n",
       "      <td>31.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>2.8387</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>755</th>\n",
       "      <td>49.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>1.7755</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>756 rows × 1558 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     feature_1  feature_2  feature_3  feature_4  feature_5  feature_6  \\\n",
       "0         60.0      468.0     7.8000        1.0          0          0   \n",
       "1        108.0      179.0     1.6574        1.0          0          0   \n",
       "2          1.0        1.0     2.0000        0.0          0          0   \n",
       "3         60.0      468.0     7.8000        1.0          0          0   \n",
       "4         60.0      120.0     2.0000        1.0          0          0   \n",
       "..         ...        ...        ...        ...        ...        ...   \n",
       "751        1.0        1.0     2.0000        0.0          0          0   \n",
       "752        1.0        1.0     2.0000        0.0          0          0   \n",
       "753       12.0      101.0     8.4166        1.0          0          0   \n",
       "754       31.0       88.0     2.8387        1.0          0          0   \n",
       "755       49.0       87.0     1.7755        0.0          0          0   \n",
       "\n",
       "     feature_7  feature_8  feature_9  feature_10  ...  feature_1549  \\\n",
       "0            0          0          0           0  ...             0   \n",
       "1            0          0          0           0  ...             0   \n",
       "2            0          0          0           0  ...             0   \n",
       "3            0          0          0           0  ...             0   \n",
       "4            0          0          0           0  ...             0   \n",
       "..         ...        ...        ...         ...  ...           ...   \n",
       "751          0          0          0           0  ...             0   \n",
       "752          0          0          0           0  ...             0   \n",
       "753          0          0          0           0  ...             0   \n",
       "754          0          0          0           0  ...             0   \n",
       "755          0          0          0           0  ...             0   \n",
       "\n",
       "     feature_1550  feature_1551  feature_1552  feature_1553  feature_1554  \\\n",
       "0               0             0             0             0             0   \n",
       "1               0             0             0             0             0   \n",
       "2               0             0             0             0             0   \n",
       "3               0             0             0             0             0   \n",
       "4               0             0             0             0             0   \n",
       "..            ...           ...           ...           ...           ...   \n",
       "751             0             0             0             0             0   \n",
       "752             0             0             0             0             0   \n",
       "753             0             0             0             0             0   \n",
       "754             0             0             0             0             0   \n",
       "755             0             0             0             0             0   \n",
       "\n",
       "     feature_1555  feature_1556  feature_1557  feature_1558  \n",
       "0               0             0             0             0  \n",
       "1               0             0             0             0  \n",
       "2               0             0             0             0  \n",
       "3               0             0             0             0  \n",
       "4               0             0             0             0  \n",
       "..            ...           ...           ...           ...  \n",
       "751             0             0             0             0  \n",
       "752             0             0             0             0  \n",
       "753             0             0             0             0  \n",
       "754             0             0             0             0  \n",
       "755             0             0             0             0  \n",
       "\n",
       "[756 rows x 1558 columns]"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = pd.read_csv('Test.csv')\n",
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_features = test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.10288989,  2.43714603,  0.88596725, ..., -0.15167089,\n",
       "        -0.12151173, -0.06311944],\n",
       "       [ 0.92764545,  0.33643624, -0.39831034, ..., -0.15167089,\n",
       "        -0.12151173, -0.06311944],\n",
       "       [-0.91087215, -0.95742653, -0.32668049, ..., -0.15167089,\n",
       "        -0.12151173, -0.06311944],\n",
       "       ...,\n",
       "       [-0.72186567, -0.23053733,  1.01488425, ..., -0.15167089,\n",
       "        -0.12151173, -0.06311944],\n",
       "       [-0.39539993, -0.32503293, -0.15132745, ..., -0.15167089,\n",
       "        -0.12151173, -0.06311944],\n",
       "       [-0.08611659, -0.33230182, -0.37361832, ..., -0.15167089,\n",
       "        -0.12151173, -0.06311944]])"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_x = StandardScaler().fit_transform(test_features) # normalizing the features\n",
    "test_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature_1</th>\n",
       "      <th>feature_2</th>\n",
       "      <th>feature_3</th>\n",
       "      <th>feature_4</th>\n",
       "      <th>feature_5</th>\n",
       "      <th>feature_6</th>\n",
       "      <th>feature_7</th>\n",
       "      <th>feature_8</th>\n",
       "      <th>feature_9</th>\n",
       "      <th>feature_10</th>\n",
       "      <th>...</th>\n",
       "      <th>feature_1549</th>\n",
       "      <th>feature_1550</th>\n",
       "      <th>feature_1551</th>\n",
       "      <th>feature_1552</th>\n",
       "      <th>feature_1553</th>\n",
       "      <th>feature_1554</th>\n",
       "      <th>feature_1555</th>\n",
       "      <th>feature_1556</th>\n",
       "      <th>feature_1557</th>\n",
       "      <th>feature_1558</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.102890</td>\n",
       "      <td>2.437146</td>\n",
       "      <td>0.885967</td>\n",
       "      <td>0.609960</td>\n",
       "      <td>-0.063119</td>\n",
       "      <td>-0.063119</td>\n",
       "      <td>-0.036394</td>\n",
       "      <td>-0.036394</td>\n",
       "      <td>-0.072932</td>\n",
       "      <td>-0.142278</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.109764</td>\n",
       "      <td>-0.089443</td>\n",
       "      <td>-0.036394</td>\n",
       "      <td>-0.063119</td>\n",
       "      <td>-0.063119</td>\n",
       "      <td>-0.089443</td>\n",
       "      <td>-0.137361</td>\n",
       "      <td>-0.151671</td>\n",
       "      <td>-0.121512</td>\n",
       "      <td>-0.063119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.927645</td>\n",
       "      <td>0.336436</td>\n",
       "      <td>-0.398310</td>\n",
       "      <td>0.609960</td>\n",
       "      <td>-0.063119</td>\n",
       "      <td>-0.063119</td>\n",
       "      <td>-0.036394</td>\n",
       "      <td>-0.036394</td>\n",
       "      <td>-0.072932</td>\n",
       "      <td>-0.142278</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.109764</td>\n",
       "      <td>-0.089443</td>\n",
       "      <td>-0.036394</td>\n",
       "      <td>-0.063119</td>\n",
       "      <td>-0.063119</td>\n",
       "      <td>-0.089443</td>\n",
       "      <td>-0.137361</td>\n",
       "      <td>-0.151671</td>\n",
       "      <td>-0.121512</td>\n",
       "      <td>-0.063119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.910872</td>\n",
       "      <td>-0.957427</td>\n",
       "      <td>-0.326680</td>\n",
       "      <td>-1.639453</td>\n",
       "      <td>-0.063119</td>\n",
       "      <td>-0.063119</td>\n",
       "      <td>-0.036394</td>\n",
       "      <td>-0.036394</td>\n",
       "      <td>-0.072932</td>\n",
       "      <td>-0.142278</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.109764</td>\n",
       "      <td>-0.089443</td>\n",
       "      <td>-0.036394</td>\n",
       "      <td>-0.063119</td>\n",
       "      <td>-0.063119</td>\n",
       "      <td>-0.089443</td>\n",
       "      <td>-0.137361</td>\n",
       "      <td>-0.151671</td>\n",
       "      <td>-0.121512</td>\n",
       "      <td>-0.063119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.102890</td>\n",
       "      <td>2.437146</td>\n",
       "      <td>0.885967</td>\n",
       "      <td>0.609960</td>\n",
       "      <td>-0.063119</td>\n",
       "      <td>-0.063119</td>\n",
       "      <td>-0.036394</td>\n",
       "      <td>-0.036394</td>\n",
       "      <td>-0.072932</td>\n",
       "      <td>-0.142278</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.109764</td>\n",
       "      <td>-0.089443</td>\n",
       "      <td>-0.036394</td>\n",
       "      <td>-0.063119</td>\n",
       "      <td>-0.063119</td>\n",
       "      <td>-0.089443</td>\n",
       "      <td>-0.137361</td>\n",
       "      <td>-0.151671</td>\n",
       "      <td>-0.121512</td>\n",
       "      <td>-0.063119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.102890</td>\n",
       "      <td>-0.092428</td>\n",
       "      <td>-0.326680</td>\n",
       "      <td>0.609960</td>\n",
       "      <td>-0.063119</td>\n",
       "      <td>-0.063119</td>\n",
       "      <td>-0.036394</td>\n",
       "      <td>-0.036394</td>\n",
       "      <td>-0.072932</td>\n",
       "      <td>-0.142278</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.109764</td>\n",
       "      <td>-0.089443</td>\n",
       "      <td>-0.036394</td>\n",
       "      <td>-0.063119</td>\n",
       "      <td>-0.063119</td>\n",
       "      <td>-0.089443</td>\n",
       "      <td>-0.137361</td>\n",
       "      <td>-0.151671</td>\n",
       "      <td>-0.121512</td>\n",
       "      <td>-0.063119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>751</th>\n",
       "      <td>-0.910872</td>\n",
       "      <td>-0.957427</td>\n",
       "      <td>-0.326680</td>\n",
       "      <td>-1.639453</td>\n",
       "      <td>-0.063119</td>\n",
       "      <td>-0.063119</td>\n",
       "      <td>-0.036394</td>\n",
       "      <td>-0.036394</td>\n",
       "      <td>-0.072932</td>\n",
       "      <td>-0.142278</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.109764</td>\n",
       "      <td>-0.089443</td>\n",
       "      <td>-0.036394</td>\n",
       "      <td>-0.063119</td>\n",
       "      <td>-0.063119</td>\n",
       "      <td>-0.089443</td>\n",
       "      <td>-0.137361</td>\n",
       "      <td>-0.151671</td>\n",
       "      <td>-0.121512</td>\n",
       "      <td>-0.063119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>752</th>\n",
       "      <td>-0.910872</td>\n",
       "      <td>-0.957427</td>\n",
       "      <td>-0.326680</td>\n",
       "      <td>-1.639453</td>\n",
       "      <td>-0.063119</td>\n",
       "      <td>-0.063119</td>\n",
       "      <td>-0.036394</td>\n",
       "      <td>-0.036394</td>\n",
       "      <td>-0.072932</td>\n",
       "      <td>-0.142278</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.109764</td>\n",
       "      <td>-0.089443</td>\n",
       "      <td>-0.036394</td>\n",
       "      <td>-0.063119</td>\n",
       "      <td>-0.063119</td>\n",
       "      <td>-0.089443</td>\n",
       "      <td>-0.137361</td>\n",
       "      <td>-0.151671</td>\n",
       "      <td>-0.121512</td>\n",
       "      <td>-0.063119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>753</th>\n",
       "      <td>-0.721866</td>\n",
       "      <td>-0.230537</td>\n",
       "      <td>1.014884</td>\n",
       "      <td>0.609960</td>\n",
       "      <td>-0.063119</td>\n",
       "      <td>-0.063119</td>\n",
       "      <td>-0.036394</td>\n",
       "      <td>-0.036394</td>\n",
       "      <td>-0.072932</td>\n",
       "      <td>-0.142278</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.109764</td>\n",
       "      <td>-0.089443</td>\n",
       "      <td>-0.036394</td>\n",
       "      <td>-0.063119</td>\n",
       "      <td>-0.063119</td>\n",
       "      <td>-0.089443</td>\n",
       "      <td>-0.137361</td>\n",
       "      <td>-0.151671</td>\n",
       "      <td>-0.121512</td>\n",
       "      <td>-0.063119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>754</th>\n",
       "      <td>-0.395400</td>\n",
       "      <td>-0.325033</td>\n",
       "      <td>-0.151327</td>\n",
       "      <td>0.609960</td>\n",
       "      <td>-0.063119</td>\n",
       "      <td>-0.063119</td>\n",
       "      <td>-0.036394</td>\n",
       "      <td>-0.036394</td>\n",
       "      <td>-0.072932</td>\n",
       "      <td>-0.142278</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.109764</td>\n",
       "      <td>-0.089443</td>\n",
       "      <td>-0.036394</td>\n",
       "      <td>-0.063119</td>\n",
       "      <td>-0.063119</td>\n",
       "      <td>-0.089443</td>\n",
       "      <td>-0.137361</td>\n",
       "      <td>-0.151671</td>\n",
       "      <td>-0.121512</td>\n",
       "      <td>-0.063119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>755</th>\n",
       "      <td>-0.086117</td>\n",
       "      <td>-0.332302</td>\n",
       "      <td>-0.373618</td>\n",
       "      <td>-1.639453</td>\n",
       "      <td>-0.063119</td>\n",
       "      <td>-0.063119</td>\n",
       "      <td>-0.036394</td>\n",
       "      <td>-0.036394</td>\n",
       "      <td>-0.072932</td>\n",
       "      <td>-0.142278</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.109764</td>\n",
       "      <td>-0.089443</td>\n",
       "      <td>-0.036394</td>\n",
       "      <td>-0.063119</td>\n",
       "      <td>-0.063119</td>\n",
       "      <td>-0.089443</td>\n",
       "      <td>-0.137361</td>\n",
       "      <td>-0.151671</td>\n",
       "      <td>-0.121512</td>\n",
       "      <td>-0.063119</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>756 rows × 1558 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     feature_1  feature_2  feature_3  feature_4  feature_5  feature_6  \\\n",
       "0     0.102890   2.437146   0.885967   0.609960  -0.063119  -0.063119   \n",
       "1     0.927645   0.336436  -0.398310   0.609960  -0.063119  -0.063119   \n",
       "2    -0.910872  -0.957427  -0.326680  -1.639453  -0.063119  -0.063119   \n",
       "3     0.102890   2.437146   0.885967   0.609960  -0.063119  -0.063119   \n",
       "4     0.102890  -0.092428  -0.326680   0.609960  -0.063119  -0.063119   \n",
       "..         ...        ...        ...        ...        ...        ...   \n",
       "751  -0.910872  -0.957427  -0.326680  -1.639453  -0.063119  -0.063119   \n",
       "752  -0.910872  -0.957427  -0.326680  -1.639453  -0.063119  -0.063119   \n",
       "753  -0.721866  -0.230537   1.014884   0.609960  -0.063119  -0.063119   \n",
       "754  -0.395400  -0.325033  -0.151327   0.609960  -0.063119  -0.063119   \n",
       "755  -0.086117  -0.332302  -0.373618  -1.639453  -0.063119  -0.063119   \n",
       "\n",
       "     feature_7  feature_8  feature_9  feature_10  ...  feature_1549  \\\n",
       "0    -0.036394  -0.036394  -0.072932   -0.142278  ...     -0.109764   \n",
       "1    -0.036394  -0.036394  -0.072932   -0.142278  ...     -0.109764   \n",
       "2    -0.036394  -0.036394  -0.072932   -0.142278  ...     -0.109764   \n",
       "3    -0.036394  -0.036394  -0.072932   -0.142278  ...     -0.109764   \n",
       "4    -0.036394  -0.036394  -0.072932   -0.142278  ...     -0.109764   \n",
       "..         ...        ...        ...         ...  ...           ...   \n",
       "751  -0.036394  -0.036394  -0.072932   -0.142278  ...     -0.109764   \n",
       "752  -0.036394  -0.036394  -0.072932   -0.142278  ...     -0.109764   \n",
       "753  -0.036394  -0.036394  -0.072932   -0.142278  ...     -0.109764   \n",
       "754  -0.036394  -0.036394  -0.072932   -0.142278  ...     -0.109764   \n",
       "755  -0.036394  -0.036394  -0.072932   -0.142278  ...     -0.109764   \n",
       "\n",
       "     feature_1550  feature_1551  feature_1552  feature_1553  feature_1554  \\\n",
       "0       -0.089443     -0.036394     -0.063119     -0.063119     -0.089443   \n",
       "1       -0.089443     -0.036394     -0.063119     -0.063119     -0.089443   \n",
       "2       -0.089443     -0.036394     -0.063119     -0.063119     -0.089443   \n",
       "3       -0.089443     -0.036394     -0.063119     -0.063119     -0.089443   \n",
       "4       -0.089443     -0.036394     -0.063119     -0.063119     -0.089443   \n",
       "..            ...           ...           ...           ...           ...   \n",
       "751     -0.089443     -0.036394     -0.063119     -0.063119     -0.089443   \n",
       "752     -0.089443     -0.036394     -0.063119     -0.063119     -0.089443   \n",
       "753     -0.089443     -0.036394     -0.063119     -0.063119     -0.089443   \n",
       "754     -0.089443     -0.036394     -0.063119     -0.063119     -0.089443   \n",
       "755     -0.089443     -0.036394     -0.063119     -0.063119     -0.089443   \n",
       "\n",
       "     feature_1555  feature_1556  feature_1557  feature_1558  \n",
       "0       -0.137361     -0.151671     -0.121512     -0.063119  \n",
       "1       -0.137361     -0.151671     -0.121512     -0.063119  \n",
       "2       -0.137361     -0.151671     -0.121512     -0.063119  \n",
       "3       -0.137361     -0.151671     -0.121512     -0.063119  \n",
       "4       -0.137361     -0.151671     -0.121512     -0.063119  \n",
       "..            ...           ...           ...           ...  \n",
       "751     -0.137361     -0.151671     -0.121512     -0.063119  \n",
       "752     -0.137361     -0.151671     -0.121512     -0.063119  \n",
       "753     -0.137361     -0.151671     -0.121512     -0.063119  \n",
       "754     -0.137361     -0.151671     -0.121512     -0.063119  \n",
       "755     -0.137361     -0.151671     -0.121512     -0.063119  \n",
       "\n",
       "[756 rows x 1558 columns]"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalised_test_features = pd.DataFrame(test_x,columns=test_features.columns)\n",
    "normalised_test_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.49630515,  0.09185567, -0.11652848],\n",
       "       [-0.922479  , -0.77390084, -0.70522466],\n",
       "       [-0.75846296, -0.4554962 , -0.39970647],\n",
       "       ...,\n",
       "       [-1.04454381, -0.90902856, -0.91307198],\n",
       "       [-0.78510504, -0.59323258, -0.59626601],\n",
       "       [-0.52070852, -0.79335363, -0.5406788 ]])"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "principalComponents_test_data = pca_train_data.fit_transform(test_x)\n",
    "principalComponents_test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.496305</td>\n",
       "      <td>0.091856</td>\n",
       "      <td>-0.116528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.922479</td>\n",
       "      <td>-0.773901</td>\n",
       "      <td>-0.705225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.758463</td>\n",
       "      <td>-0.455496</td>\n",
       "      <td>-0.399706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.090920</td>\n",
       "      <td>5.307219</td>\n",
       "      <td>0.853872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.106260</td>\n",
       "      <td>12.196516</td>\n",
       "      <td>-0.953055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>751</th>\n",
       "      <td>-0.699218</td>\n",
       "      <td>-0.248916</td>\n",
       "      <td>-0.367674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>752</th>\n",
       "      <td>-0.946506</td>\n",
       "      <td>-1.005222</td>\n",
       "      <td>-1.031742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>753</th>\n",
       "      <td>-1.044544</td>\n",
       "      <td>-0.909029</td>\n",
       "      <td>-0.913072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>754</th>\n",
       "      <td>-0.785105</td>\n",
       "      <td>-0.593233</td>\n",
       "      <td>-0.596266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>755</th>\n",
       "      <td>-0.520709</td>\n",
       "      <td>-0.793354</td>\n",
       "      <td>-0.540679</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>756 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0          1         2\n",
       "0   -0.496305   0.091856 -0.116528\n",
       "1   -0.922479  -0.773901 -0.705225\n",
       "2   -0.758463  -0.455496 -0.399706\n",
       "3    2.090920   5.307219  0.853872\n",
       "4    2.106260  12.196516 -0.953055\n",
       "..        ...        ...       ...\n",
       "751 -0.699218  -0.248916 -0.367674\n",
       "752 -0.946506  -1.005222 -1.031742\n",
       "753 -1.044544  -0.909029 -0.913072\n",
       "754 -0.785105  -0.593233 -0.596266\n",
       "755 -0.520709  -0.793354 -0.540679\n",
       "\n",
       "[756 rows x 3 columns]"
      ]
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca_normalised_test_features = pd.DataFrame(principalComponents_test_data)\n",
    "pca_normalised_test_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = model.predict(pca_normalised_test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "       0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "       0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_data=pd.read_csv('Sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_data['Class']=prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_data.to_csv('Submission11_NN_PCA.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
